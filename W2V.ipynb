{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect whether this is a google environment\n",
    "\n",
    "COLAB_ENVIRONMENT = False\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    COLAB_ENVIRONMENT = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "if COLAB_ENVIRONMENT:\n",
    "    py_file_location = \"./drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch\" # my private packages are stored here\n",
    "    home_directory = './drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/' # my home directory is stored in ./LAB of google drive\n",
    "    !pip install einops\n",
    "else:\n",
    "    py_file_location = './PrivatePackages/pytorch'\n",
    "    home_directory = './'\n",
    "\n",
    "sys.path.append(os.path.abspath(py_file_location))\n",
    "\n",
    "from environment import *\n",
    "from utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_class import LSTM, BERT, LSTM_DANN, BERT_DANN, LSTM_DCE_DANN, BERT_DCE_DANN, LSTM_Hinge, BERT_Hinge, W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "with open(home_directory + '/data/raw/comp90051-2024s1-project-1/domain1_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data1.append(json.loads(line))\n",
    "\n",
    "data2 = []\n",
    "with open(home_directory + './data/raw/comp90051-2024s1-project-1/domain2_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data2.append(json.loads(line))\n",
    "\n",
    "data_test = []\n",
    "with open(home_directory + '/data/raw/comp90051-2024s1-project-1/test_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data_test.append(json.loads(line))\n",
    "\n",
    "# create domain labels for data\n",
    "for i in range(len(data1)):\n",
    "    data1[i]['domain'] = 0\n",
    "for i in range(len(data2)):\n",
    "    data2[i]['domain'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val Test Split\n",
    "\n",
    "# get labels for stratification\n",
    "label1 = [instance['label'] for instance in data1]\n",
    "label2 = [instance['label'] for instance in data2]\n",
    "\n",
    "train_ix_1, val_test_ix_1 = train_test_split(range(len(data1)), test_size=0.3, random_state=SEED, stratify = label1)\n",
    "train_ix_2, val_test_ix_2 = train_test_split(range(len(data2)), test_size=0.3, random_state=SEED, stratify = label2)\n",
    "val_ix_1, test_ix_1 = train_test_split(val_test_ix_1, test_size=0.5, random_state=SEED, stratify = [data1[i]['label'] for i in val_test_ix_1])\n",
    "val_ix_2, test_ix_2 = train_test_split(val_test_ix_2, test_size=0.5, random_state=SEED, stratify = [data2[i]['label'] for i in val_test_ix_2])\n",
    "\n",
    "# split data according to the index from train_test_split\n",
    "train_data_1 = [data1[i] for i in train_ix_1]\n",
    "val_data_1 = [data1[i] for i in val_ix_1]\n",
    "test_data_1 = [data1[i] for i in test_ix_1]\n",
    "train_data_2 = [data2[i] for i in train_ix_2]\n",
    "val_data_2 = [data2[i] for i in val_ix_2]\n",
    "test_data_2 = [data2[i] for i in test_ix_2]\n",
    "\n",
    "# combine the data\n",
    "train_data = train_data_1 + train_data_2\n",
    "val_data = val_data_1 + val_data_2\n",
    "test_data = test_data_1 + test_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [00:00<00:00, 27317.66it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 88180.81it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 125557.08it/s]\n",
      "100%|██████████| 4000/4000 [00:00<00:00, 91272.29it/s]\n",
      "100%|██████████| 12600/12600 [00:00<00:00, 39957.50it/s]\n",
      "100%|██████████| 12600/12600 [00:00<00:00, 44726.71it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 44775.33it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 45783.61it/s]\n",
      "100%|██████████| 4000/4000 [00:00<00:00, 45908.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class prior: 0.2222222222222222 0.7777777777777778\n",
      "domain prior: 0.7222222222222222 0.2777777777777778\n",
      "dom1 class prior: 0.5 0.5\n",
      "dom2 class prior: 0.11538461538461539 0.8846153846153846\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 256\n",
    "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
    "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
    "LOW_FREQ_TOKEN = False\n",
    "CLS = True\n",
    "PAD_FRONT = False\n",
    "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
    "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
    "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
    "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
    "                                                              cropped_val_data, \\\n",
    "                                                                cropped_test_data, \\\n",
    "                                                                    cropped_future_data, \\\n",
    "                                                                        MAX_SENTENCE_LENGTH, \\\n",
    "                                                                            raw_token_pytorch_map, \\\n",
    "                                                                                CLS=CLS, \\\n",
    "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
    "                                                                                        pad_front=PAD_FRONT)\n",
    "pos_prior, neg_prior = get_distribution(train_y)\n",
    "print('class prior:', pos_prior, neg_prior)\n",
    "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
    "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
    "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
    "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
    "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
    "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
    "# pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
    "\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_DataFactory(data: list, context_window: int, seed: int, raw_token_pytorch_map: dict, k) -> list:\n",
    "\n",
    "    \"\"\" Get W2V training data \"\"\"\n",
    "    \n",
    "    assert context_window % 2 == 1, 'context window must be odd'\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    MAX_SAMPLED_NEGATIVE_TOKENS = 10000\n",
    "\n",
    "    retokenised_keys = list(raw_token_pytorch_map.keys())\n",
    "\n",
    "    negative_tokens = np.random.choice(retokenised_keys, MAX_SAMPLED_NEGATIVE_TOKENS)\n",
    "    negative_tokens = [x if x[0].isalpha() else int(x) for x in negative_tokens]\n",
    "\n",
    "    negative_up_to = 0\n",
    "\n",
    "    w2v_data = []\n",
    "\n",
    "    for instance in tqdm(data): # every sentence\n",
    "        tokens = [context_window//2 * 'CLS'] + instance['text'] + [context_window//2 * raw_token_pytorch_map['PAD']]\n",
    "\n",
    "        for i in range(context_window//2, len(tokens) - context_window//2): # avoid pad and cls # every token\n",
    "            \n",
    "            focus_token_retokenised = raw_token_pytorch_map.get(tokens[i], raw_token_pytorch_map['UNK'])\n",
    "            \n",
    "            context_words = dict()\n",
    "\n",
    "            for j in range(-context_window//2+1, context_window//2+1):\n",
    "                if j != 0:\n",
    "                    context_words[tokens[i+j]] = 0\n",
    "\n",
    "            for j in range(-context_window//2+1, context_window//2+1): # every neighbour in window\n",
    "                if j != 0: # don't want to make positive sample with self\n",
    "                    if context_words.get(tokens[i+j], 0): # CLS and Padding (being start and end) being repeated\n",
    "                        continue \n",
    "                    else:\n",
    "                        context_words[tokens[i+j]] = 1\n",
    "                    \n",
    "                    mask = [raw_token_pytorch_map.get(tokens[i+j], raw_token_pytorch_map['UNK'])]\n",
    "                    for _ in range(k): # sample the same number of negatives\n",
    "                        \n",
    "                        while True:\n",
    "                            \n",
    "                            if negative_up_to == MAX_SAMPLED_NEGATIVE_TOKENS:\n",
    "                                negative_up_to = 0\n",
    "\n",
    "                            sampled_negative_retokenised = negative_tokens[negative_up_to]\n",
    "                            negative_up_to += 1\n",
    "\n",
    "                            if sampled_negative_retokenised not in context_words: # didn't sample a positive case\n",
    "                                mask.append(raw_token_pytorch_map[sampled_negative_retokenised])\n",
    "                                break\n",
    "\n",
    "                    \n",
    "                    new_instance = {'token': focus_token_retokenised, 'mask': mask}\n",
    "                    w2v_data.append(new_instance)\n",
    "    \n",
    "    return w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [01:00<00:00, 207.00it/s]\n",
      "100%|██████████| 2700/2700 [00:03<00:00, 696.21it/s]\n"
     ]
    }
   ],
   "source": [
    "w2v_data = W2V_DataFactory(train_data, 3, SEED, raw_token_pytorch_map, 4)\n",
    "val_w2v_data = W2V_DataFactory(val_data, 3, SEED, raw_token_pytorch_map, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [x['token'] for x in w2v_data]\n",
    "train_mask = [x['mask'] for x in w2v_data]\n",
    "\n",
    "val_x = [x['token'] for x in w2v_data]\n",
    "val_mask = [x['mask'] for x in val_w2v_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = train_x[:100]\n",
    "# train_mask = train_mask[:100]\n",
    "\n",
    "# val_x = val_x[:100]\n",
    "# val_mask = val_mask[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7400/765490 [01:19<2:15:44, 93.08it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m W2V(W2V_Config) \u001b[38;5;66;03m# initialise the model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# train the model (all cells except this one will print training log and evaluation at each batch)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 88\u001b[0m, in \u001b[0;36mW2V_Model.fit\u001b[0;34m(self, total_train_X, total_train_mask, total_val_X, total_val_mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 88\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m     91\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_criterion(pred, target)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 44\u001b[0m, in \u001b[0;36mW2V.Model.forward\u001b[0;34m(self, X, mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(embed)\n\u001b[1;32m     40\u001b[0m prediction \u001b[38;5;241m=\u001b[39m prediction[dim_1_mask, dim_2_mask]\n\u001b[0;32m---> 44\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mprediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BERT - CELoss\n",
    "\n",
    "class W2V_Config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 256\n",
    "    k=5\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 8\n",
    "    epochs = 32\n",
    "    lr = 1e-5\n",
    "    patience = 10\n",
    "    pretrain_loss = nn.CrossEntropyLoss()\n",
    "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
    "    regularisation_loss = None\n",
    "    scheduler = False\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory + './'\n",
    "    saving_address = home_directory + f'./results/'\n",
    "    name = f'W2V_Pretrain_Embeddings'\n",
    "\n",
    "model = W2V(W2V_Config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_mask, val_x, val_mask)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_mask, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP90051",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
