{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYRTCgGLL-s-"
      },
      "source": [
        "# Pytorch Training UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaPEVpXNL-tC"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5xuQPsyL-tD",
        "outputId": "9f22e09e-d9ee-4be3-823f-b1e746c23356"
      },
      "outputs": [],
      "source": [
        "# detect whether this is a google environment\n",
        "\n",
        "COLAB_ENVIRONMENT = False\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    COLAB_ENVIRONMENT = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "if COLAB_ENVIRONMENT:\n",
        "    py_file_location = \"./drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch\" # my private packages are stored here\n",
        "    home_directory = './drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/' # my home directory is stored in ./LAB of google drive\n",
        "    !pip install einops\n",
        "else:\n",
        "    py_file_location = './PrivatePackages/pytorch'\n",
        "    home_directory = './'\n",
        "\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "from environment import *\n",
        "from utils import *\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from model.model_class import LSTM, BERT, LSTM_DANN, BERT_DANN, LSTM_DCE_DANN, BERT_DCE_DANN, BERT_Hinge, W2V, \\\n",
        "BERT_DCE_DANN_DoubleDecoder, BERT_DoubleDecoder, BERT_DANN_DoubleDecoder, LSTM_DCE_DANN_DoubleDecoder, LSTM_DoubleDecoder, LSTM_DANN_DoubleDecoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k-bq7crL-tE"
      },
      "source": [
        "### Set Seed and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bo5HdEOrL-tF"
      },
      "outputs": [],
      "source": [
        "SEED = 2608\n",
        "\n",
        "data1 = []\n",
        "with open(home_directory + '/data/curated/comp90051-2024s1-project-1/domain1_train_data.json', 'r') as f:\n",
        "    for line in f:\n",
        "        data1.append(json.loads(line))\n",
        "\n",
        "data2 = []\n",
        "with open(home_directory + './data/curated/comp90051-2024s1-project-1/domain2_train_data.json', 'r') as f:\n",
        "    for line in f:\n",
        "        data2.append(json.loads(line))\n",
        "\n",
        "data_test = []\n",
        "with open(home_directory + '/data/curated/comp90051-2024s1-project-1/test_data.json', 'r') as f:\n",
        "    for line in f:\n",
        "        data_test.append(json.loads(line))\n",
        "\n",
        "# create domain labels for data\n",
        "for i in range(len(data1)):\n",
        "    data1[i]['domain'] = 0\n",
        "for i in range(len(data2)):\n",
        "    data2[i]['domain'] = 1\n",
        "\n",
        "\n",
        "# Train Val Test Split\n",
        "# get labels for stratification\n",
        "label1 = [instance['label'] for instance in data1]\n",
        "label2 = [instance['label'] for instance in data2]\n",
        "\n",
        "train_ix_1, val_test_ix_1 = train_test_split(range(len(data1)), test_size=0.3, random_state=SEED, stratify = label1)\n",
        "train_ix_2, val_test_ix_2 = train_test_split(range(len(data2)), test_size=0.3, random_state=SEED, stratify = label2)\n",
        "val_ix_1, test_ix_1 = train_test_split(val_test_ix_1, test_size=0.5, random_state=SEED, stratify = [data1[i]['label'] for i in val_test_ix_1])\n",
        "val_ix_2, test_ix_2 = train_test_split(val_test_ix_2, test_size=0.5, random_state=SEED, stratify = [data2[i]['label'] for i in val_test_ix_2])\n",
        "\n",
        "# split data according to the index from train_test_split\n",
        "train_data_1 = [data1[i] for i in train_ix_1]\n",
        "val_data_1 = [data1[i] for i in val_ix_1]\n",
        "test_data_1 = [data1[i] for i in test_ix_1]\n",
        "train_data_2 = [data2[i] for i in train_ix_2]\n",
        "val_data_2 = [data2[i] for i in val_ix_2]\n",
        "test_data_2 = [data2[i] for i in test_ix_2]\n",
        "\n",
        "# combine the data\n",
        "train_data = train_data_1 + train_data_2\n",
        "val_data = val_data_1 + val_data_2\n",
        "test_data = test_data_1 + test_data_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Load Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # BERT - CELoss\n",
        "\n",
        "# MAX_SENTENCE_LENGTH = 256\n",
        "# MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "# MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "# LOW_FREQ_TOKEN = False\n",
        "# CLS = True\n",
        "# PAD_FRONT = False\n",
        "# W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "# cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "# cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "# train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "#                                                               cropped_val_data, \\\n",
        "#                                                                 cropped_test_data, \\\n",
        "#                                                                     cropped_future_data, \\\n",
        "#                                                                         MAX_SENTENCE_LENGTH, \\\n",
        "#                                                                             raw_token_pytorch_map, \\\n",
        "#                                                                                 CLS=CLS, \\\n",
        "#                                                                                     low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "#                                                                                         pad_front=PAD_FRONT)\n",
        "# pos_prior, neg_prior = get_distribution(train_y)\n",
        "# print('class prior:', pos_prior, neg_prior)\n",
        "# pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "# print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "# dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "# print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "# dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "# print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "\n",
        "# class W2V_Config:\n",
        "#     # ----------------- architectual hyperparameters ----------------- #\n",
        "#     d_model = 256\n",
        "#     k=5\n",
        "#     # ----------------- optimisation hyperparameters ----------------- #\n",
        "#     random_state = SEED\n",
        "#     batch_size = 8\n",
        "#     epochs = 32\n",
        "#     lr = 1e-5\n",
        "#     patience = 10\n",
        "#     pretrain_loss = nn.CrossEntropyLoss()\n",
        "#     pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "#     regularisation_loss = None\n",
        "#     scheduler = False\n",
        "#     grad_clip = False\n",
        "#     train_embedding = False\n",
        "#     # ----------------- operation hyperparameters ----------------- #\n",
        "#     n_unique_tokens = len(raw_token_pytorch_map)\n",
        "#     # ----------------- saving hyperparameters ----------------- #\n",
        "#     rootpath = home_directory + './'\n",
        "#     saving_address = home_directory + f'./results/'\n",
        "#     name = f'W2V_Pretrain_Embeddings-pretrained'\n",
        "\n",
        "# pretrained = W2V(W2V_Config) # initialise the model\n",
        "\n",
        "# pretrained.load()\n",
        "\n",
        "# w2v_embed = pretrained.model.embed.weight\n",
        "# w2v_combined_embed = (pretrained.model.embed.weight + pretrained.model.linear.weight)/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAX_SENTENCE_LENGTH = 256\n",
        "# MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "# MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "# LOW_FREQ_TOKEN = False\n",
        "# CLS = True\n",
        "# PAD_FRONT = False\n",
        "# W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "# cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "# cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "# train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "#                                                               cropped_val_data, \\\n",
        "#                                                                 cropped_test_data, \\\n",
        "#                                                                     cropped_future_data, \\\n",
        "#                                                                         MAX_SENTENCE_LENGTH, \\\n",
        "#                                                                             raw_token_pytorch_map, \\\n",
        "#                                                                                 CLS=CLS, \\\n",
        "#                                                                                     low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "#                                                                                         pad_front=PAD_FRONT)\n",
        "# pos_prior, neg_prior = get_distribution(train_y)\n",
        "# print('class prior:', pos_prior, neg_prior)\n",
        "# pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "# print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "# dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "# print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "# dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "# print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "# # pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "# print('---')\n",
        "\n",
        "# class BERT_config:\n",
        "#     # ----------------- architectual hyperparameters ----------------- #\n",
        "#     d_model = 256\n",
        "#     d_ff = 1024 # = 4* d_model\n",
        "#     n_heads = 8\n",
        "#     dropout = 0.1\n",
        "#     e_layers = 8\n",
        "#     embedding_aggregation = 'cls' # TODO\n",
        "#     n_mlp_layers = 1\n",
        "#     res_learning = False\n",
        "#     activation = nn.ReLU()\n",
        "#     mask_flag = False # causal mask\n",
        "#     train_embedding = True\n",
        "#     # ----------------- optimisation hyperparameters ----------------- #\n",
        "#     random_state = SEED\n",
        "#     batch_size = 8\n",
        "#     epochs = 32\n",
        "#     lr = 1e-5\n",
        "#     patience = 10\n",
        "#     loss = nn.CrossEntropyLoss()\n",
        "#     # loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "#     validation_loss = nn.CrossEntropyLoss()\n",
        "#     # validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "#     pretrain_loss = nn.CrossEntropyLoss()\n",
        "#     pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "#     regularisation_loss = None\n",
        "#     scheduler = False\n",
        "#     grad_clip = False\n",
        "#     # ----------------- operation hyperparameters ----------------- #\n",
        "#     d_output = 2\n",
        "#     seq_len = MAX_SENTENCE_LENGTH\n",
        "#     n_unique_tokens = len(raw_token_pytorch_map)\n",
        "#     # ----------------- saving hyperparameters ----------------- #\n",
        "#     rootpath = home_directory + './'\n",
        "#     saving_address = home_directory + f'./results/'\n",
        "#     name = f'BERT_Classifier_pretrained'\n",
        "\n",
        "# pretrained_bert = BERT(BERT_config) # initialise the model\n",
        "# pretrained_bert.load()\n",
        "\n",
        "# pretrained_bert_embed = pretrained_bert.model.embedding.embedding.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20T2YmtZL-tI"
      },
      "source": [
        "---\n",
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVDueBf3L-tJ"
      },
      "source": [
        "\n",
        "#### 1. DANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUX_FEATURES = ['perplexity', 'burstiness', 'length', 'unique_word_ratio', 'domain']\n",
        "MAX_SENTENCE_LENGTH = 128\n",
        "MIN_FREQUENCY = 0 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x, future_dom = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "train_aux, val_aux, test_aux, future_aux = Data_Factory_aux(cropped_train_data, cropped_val_data, cropped_test_data, cropped_future_data, AUX_FEATURES)\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class LSTM_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 512\n",
        "    n_recurrent_layers = 2\n",
        "    bidirectional = False\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    flatten = False\n",
        "    activation = nn.ReLU()\n",
        "    res_learning = True\n",
        "    mask_flag = False # TODO\n",
        "    train_embedding = True\n",
        "    d_extra_decoder_features = len(AUX_FEATURES)\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 128\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 5\n",
        "    loss = nn.BCELoss()\n",
        "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    validation_loss = nn.BCELoss()\n",
        "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    domain_loss = nn.BCELoss()\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    alpha = 0.1\n",
        "    scheduler = True\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    optimised_metric = 'dom1' # 'global', 'dom1', 'dom2'\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory +  f'./results/'\n",
        "    name = f'LSTM_DANN'\n",
        "\n",
        "model = LSTM_DANN(LSTM_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "else:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom, train_aux = train_aux, val_aux = val_aux)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)\n",
        "else:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, aux= val_aux, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, aux=test_aux, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUX_FEATURES = ['perplexity', 'burstiness', 'length', 'unique_word_ratio', 'domain']\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x, future_dom = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "train_aux, val_aux, test_aux, future_aux = Data_Factory_aux(cropped_train_data, cropped_val_data, cropped_test_data, cropped_future_data, AUX_FEATURES)\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    d_extra_decoder_features = len(AUX_FEATURES)\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    # loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    validation_loss = nn.CrossEntropyLoss()\n",
        "    # validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    # domain_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_dom_prior, 2*neg_dom_prior]))\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    optimised_metric = 'dom1' # 'global', 'dom1', 'dom2'\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_DANN'\n",
        "\n",
        "model = BERT_DANN(BERT_DANN_config) # initialise the model\n",
        "\n",
        "## PRETRAIN\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_combined_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(pretrained_bert_embed, freeze=False)\n",
        "# model.model.encoder = pretrained_bert.model.encoder\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "else:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom, train_aux = train_aux, val_aux = val_aux)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)\n",
        "else:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, aux= val_aux, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, aux=test_aux, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srUFUxEV0jK0",
        "outputId": "e696fc9a-4a55-42d5-a9cd-9dba2af2d9ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12600/12600 [00:00<00:00, 15485.68it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 25772.33it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 3127.14it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 41878.65it/s]\n",
            "100%|██████████| 12600/12600 [00:01<00:00, 12264.92it/s]\n",
            "100%|██████████| 12600/12600 [00:00<00:00, 37174.52it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 40789.31it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 35825.73it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 39010.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class prior: 0.2222222222222222 0.7777777777777778\n",
            "domain prior: 0.7222222222222222 0.2777777777777778\n",
            "dom1 class prior: 0.5 0.5\n",
            "dom2 class prior: 0.11538461538461539 0.8846153846153846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12600/12600 [00:02<00:00, 4654.85it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 6566.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Epoch 21 Val   | Classification Loss:  0.1625 | Accuracy:  0.8574| F1:  0.7150 | Balanced Accuracy:  0.8387 | Dom Avg Accuracy:  0.8010 |\n",
            "                            Domain Loss:  0.5397 | Domain Accuracy:  0.7581 |  \n",
            "                            Domain 1 Accuracy:  0.7920| Domain 1 F1:  0.8050 | Domain 1 Balanced Accuracy:  0.7920 |  \n",
            "                            Domain 2 Accuracy:  0.8826| Domain 2 F1:  0.5844 | Domain 2 Balanced Accuracy:  0.8100\n",
            "Epoch 21 Val   | Classification Loss:  0.1658 | Accuracy:  0.8589| F1:  0.7167 | Balanced Accuracy:  0.8390 | Dom Avg Accuracy:  0.8013 |\n",
            "                            Domain Loss:  0.5434 | Domain Accuracy:  0.7607 |  \n",
            "                            Domain 1 Accuracy:  0.7667| Domain 1 F1:  0.7804 | Domain 1 Balanced Accuracy:  0.7667 |  \n",
            "                            Domain 2 Accuracy:  0.8944| Domain 2 F1:  0.6241 | Domain 2 Balanced Accuracy:  0.8359\n"
          ]
        }
      ],
      "source": [
        "# EXPERIMENT_NAME = '8bert_1mlpc_1mlpd_256d1024_256d40_8h_0.1_embed_4batch_bwce_low_freq'\n",
        "\n",
        "# future_pred_y, future_pred_dom = model.predict(future_x)\n",
        "\n",
        "# future_pred_y = [1 if x[1] > x[0] else 0 for x in future_pred_y]\n",
        "\n",
        "# predictions = pd.DataFrame({'id': range(len(future_pred_y)), 'class': future_pred_y})\n",
        "# predictions.to_csv(home_directory + f'predictions/{EXPERIMENT_NAME}_classification.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycRT4iArPckp"
      },
      "source": [
        "---\n",
        "#### 2. DCE_DANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUX_FEATURES = ['perplexity', 'burstiness', 'length', 'unique_word_ratio', 'domain']\n",
        "MAX_SENTENCE_LENGTH = 128\n",
        "MIN_FREQUENCY = 0 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x, future_dom = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "train_aux, val_aux, test_aux, future_aux = Data_Factory_aux(cropped_train_data, cropped_val_data, cropped_test_data, cropped_future_data, AUX_FEATURES)\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class LSTM_DCE_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 512\n",
        "    n_recurrent_layers = 2\n",
        "    bidirectional = False\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    flatten = False\n",
        "    activation = nn.ReLU()\n",
        "    res_learning = True\n",
        "    mask_flag = False # TODO\n",
        "    train_embedding = True\n",
        "    d_extra_decoder_features = len(AUX_FEATURES)\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 128\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 5\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    domain_1_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom1_pos_prior, 2*dom1_neg_prior]))\n",
        "    domain_1_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom1_pos_prior, 2*dom1_neg_prior]))\n",
        "    domain_2_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom2_pos_prior, 2*dom2_neg_prior]))\n",
        "    domain_2_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom2_pos_prior, 2*dom2_neg_prior]))\n",
        "    domain_prior = [2*pos_dom_prior, 2*neg_dom_prior]\n",
        "    # domain_prior = [1, 1]\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    # domain_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_dom_prior, 2*neg_dom_prior]))\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    alpha = 0.1\n",
        "    scheduler = True\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    optimised_metric = 'dom1' # 'global', 'dom1', 'dom2'\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory +  f'./results/'\n",
        "    name = f'LSTM_DCE_DANN'\n",
        "\n",
        "model = LSTM_DCE_DANN(LSTM_DCE_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "else:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom, train_aux = train_aux, val_aux = val_aux)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)\n",
        "else:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, aux= val_aux, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, aux=test_aux, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YQC0zod66Xu",
        "outputId": "01e231cc-2549-4d60-f950-19080dbe2275"
      },
      "outputs": [],
      "source": [
        "AUX_FEATURES = ['perplexity', 'burstiness', 'length', 'unique_word_ratio', 'domain']\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x, future_dom = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "train_aux, val_aux, test_aux, future_aux = Data_Factory_aux(cropped_train_data, cropped_val_data, cropped_test_data, cropped_future_data, AUX_FEATURES)\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DCE_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    d_extra_decoder_features = len(AUX_FEATURES)\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    optimised_metric = 'global' # 'dom1', 'dom2'\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    domain_1_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom1_pos_prior, 2*dom1_neg_prior]))\n",
        "    domain_1_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom1_pos_prior, 2*dom1_neg_prior]))\n",
        "    domain_2_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom2_pos_prior, 2*dom2_neg_prior]))\n",
        "    domain_2_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom2_pos_prior, 2*dom2_neg_prior]))\n",
        "    domain_prior = [2*pos_dom_prior, 2*neg_dom_prior]\n",
        "    # domain_prior = [1, 1]\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    # domain_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_dom_prior, 2*neg_dom_prior]))\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_Classifier'\n",
        "\n",
        "model = BERT_DCE_DANN(BERT_DCE_DANN_config) # initialise the model\n",
        "\n",
        "## PRETRAIN\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_combined_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(pretrained_bert_embed, freeze=False)\n",
        "# model.model.encoder = pretrained_bert.model.encoder\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "else:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom, train_aux = train_aux, val_aux = val_aux)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)\n",
        "else:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, aux= val_aux, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, aux=test_aux, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#### 3. Double Decoder DANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUX_FEATURES = ['perplexity', 'burstiness', 'length', 'unique_word_ratio', 'domain']\n",
        "MAX_SENTENCE_LENGTH = 128\n",
        "MIN_FREQUENCY = 0 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x, future_dom = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "train_aux, val_aux, test_aux, future_aux = Data_Factory_aux(cropped_train_data, cropped_val_data, cropped_test_data, cropped_future_data, AUX_FEATURES)\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class LSTM_DANN_DoubleDecoder_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 512\n",
        "    n_recurrent_layers = 2\n",
        "    bidirectional = False\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    flatten = False\n",
        "    activation = nn.ReLU()\n",
        "    res_learning = True\n",
        "    mask_flag = False # TODO\n",
        "    train_embedding = True\n",
        "    d_extra_decoder_features = len(AUX_FEATURES)\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 128\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 5\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    # loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    validation_loss = nn.CrossEntropyLoss()\n",
        "    # validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    # domain_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_dom_prior, 2*neg_dom_prior]))\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    alpha = 0.1\n",
        "    scheduler = True\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    optimised_metric = 'dom1' # 'global', 'dom1', 'dom2'\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory +  f'./results/'\n",
        "    name = f'LSTM_DANN_DoubleDecoder'\n",
        "\n",
        "model = LSTM_DANN_DoubleDecoder(LSTM_DANN_DoubleDecoder_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "else:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom, train_aux = train_aux, val_aux = val_aux)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)\n",
        "else:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, aux= val_aux, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, aux=test_aux, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUX_FEATURES = ['perplexity', 'burstiness', 'length', 'unique_word_ratio', 'domain']\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x, future_dom = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "train_aux, val_aux, test_aux, future_aux = Data_Factory_aux(cropped_train_data, cropped_val_data, cropped_test_data, cropped_future_data, AUX_FEATURES)\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "\n",
        "train_x = train_x[:10]\n",
        "train_y = train_y[:10]\n",
        "train_dom = train_dom[:10]\n",
        "train_aux = train_aux[:10]\n",
        "val_x = val_x[:10]\n",
        "val_y = val_y[:10]\n",
        "val_dom = val_dom[:10]\n",
        "val_aux = val_aux[:10]\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DANN_DoubleDecoder_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    d_extra_decoder_features = len(AUX_FEATURES)\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    # loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    validation_loss = nn.CrossEntropyLoss()\n",
        "    # validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    # domain_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_dom_prior, 2*neg_dom_prior]))\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    optimised_metric = 'dom1' # 'global', 'dom1', 'dom2'\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_DANN_DoubleDecoder'\n",
        "\n",
        "model = BERT_DANN_DoubleDecoder(BERT_DANN_DoubleDecoder_config) # initialise the model\n",
        "\n",
        "## PRETRAIN\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_combined_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(pretrained_bert_embed, freeze=False)\n",
        "# model.model.encoder = pretrained_bert.model.encoder\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "else:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom, train_aux = train_aux, val_aux = val_aux)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)\n",
        "else:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, aux= val_aux, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, aux=test_aux, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#### 4. Double Decoder DANN_DCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUX_FEATURES = ['perplexity', 'burstiness', 'length', 'unique_word_ratio', 'domain']\n",
        "MAX_SENTENCE_LENGTH = 128\n",
        "MIN_FREQUENCY = 0 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x, future_dom = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "train_aux, val_aux, test_aux, future_aux = Data_Factory_aux(cropped_train_data, cropped_val_data, cropped_test_data, cropped_future_data, AUX_FEATURES)\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class LSTM_DANN_DCE_DoubleDecoder_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 512\n",
        "    n_recurrent_layers = 2\n",
        "    bidirectional = False\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    flatten = False\n",
        "    activation = nn.ReLU()\n",
        "    res_learning = True\n",
        "    mask_flag = False # TODO\n",
        "    train_embedding = True\n",
        "    d_extra_decoder_features = len(AUX_FEATURES)\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 128\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 5\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    domain_1_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom1_pos_prior, 2*dom1_neg_prior]))\n",
        "    domain_1_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom1_pos_prior, 2*dom1_neg_prior]))\n",
        "    domain_2_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom2_pos_prior, 2*dom2_neg_prior]))\n",
        "    domain_2_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom2_pos_prior, 2*dom2_neg_prior]))\n",
        "    domain_prior = [2*pos_dom_prior, 2*neg_dom_prior]\n",
        "    # domain_prior = [1, 1]\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    # domain_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_dom_prior, 2*neg_dom_prior]))\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    alpha = 0.1\n",
        "    scheduler = True\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    optimised_metric = 'dom1' # 'global', 'dom1', 'dom2'\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory +  f'./results/'\n",
        "    name = f'LSTM_DANN_DoubleDecoder'\n",
        "\n",
        "model = LSTM_DANN_DCE_DoubleDecoder(LSTM_DANN_DCE_DoubleDecoder_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "else:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom, train_aux = train_aux, val_aux = val_aux)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)\n",
        "else:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, aux= val_aux, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, aux=test_aux, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT DBCE - celoss expr - just bal domain\n",
        "\n",
        "AUX_FEATURES = ['perplexity', 'burstiness', 'length', 'unique_word_ratio', 'domain']\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x, future_dom = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "train_aux, val_aux, test_aux, future_aux = Data_Factory_aux(cropped_train_data, cropped_val_data, cropped_test_data, cropped_future_data, AUX_FEATURES)\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DCE_DANN_DoubleDecoder_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    d_extra_decoder_features = len(AUX_FEATURES)\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    optimised_metric = 'global' # 'dom1', 'dom2'\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_prior, 2*neg_prior]))\n",
        "    domain_1_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom1_pos_prior, 2*dom1_neg_prior]))\n",
        "    domain_1_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom1_pos_prior, 2*dom1_neg_prior]))\n",
        "    domain_2_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom2_pos_prior, 2*dom2_neg_prior]))\n",
        "    domain_2_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*dom2_pos_prior, 2*dom2_neg_prior]))\n",
        "    domain_prior = [2*pos_dom_prior, 2*neg_dom_prior]\n",
        "    # domain_prior = [1, 1]\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    # domain_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([2*pos_dom_prior, 2*neg_dom_prior]))\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_DCE_DANN_DoubleDecoder'\n",
        "\n",
        "model = BERT_DCE_DANN_DoubleDecoder(BERT_DCE_DANN_DoubleDecoder_config) # initialise the model\n",
        "\n",
        "## PRETRAIN\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_combined_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(pretrained_bert_embed, freeze=False)\n",
        "# model.model.encoder = pretrained_bert.model.encoder\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "else:\n",
        "    best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom, train_aux = train_aux, val_aux = val_aux)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "if len(AUX_FEATURES) == 0:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)\n",
        "else:\n",
        "    model.eval(val_x, val_y, val_dom, best_epoch, aux= val_aux, evaluation_mode = True)\n",
        "    model.eval(test_x, test_y, test_dom, best_epoch, aux=test_aux, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ6O6vc_RDlN"
      },
      "source": [
        "---\n",
        "# Hinge Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKgxTV4fQ_ck"
      },
      "outputs": [],
      "source": [
        "# BERT - hinge work in progress\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "# pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_Hinge_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 6\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_layers = 1\n",
        "    # n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-3\n",
        "    patience = 10\n",
        "    # loss = nn.BCELoss()\n",
        "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    loss = HingeLoss()\n",
        "    # validation_loss = nn.BCELoss()\n",
        "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    validation_loss = HingeLoss()\n",
        "    domain_loss = nn.BCELoss()\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 1\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_Hinge'\n",
        "\n",
        "model = BERT_Hinge(BERT_Hinge_config) # initialise the model\n",
        "\n",
        "# BERT - DANN WCELoss\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "# pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_DANN'\n",
        "\n",
        "model = BERT_DANN(BERT_DANN_config) # initialise the model\n",
        "\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_combined_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(w2v_embed, freeze=False)\n",
        "# model.model.embedding.embedding = nn.Embedding.from_pretrained(pretrained_bert_embed, freeze=False)\n",
        "\n",
        "# model.model.encoder = pretrained_bert.model.encoder\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Historic Best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # BERT - SoftmaxBCELoss+OptBalAccu\n",
        "\n",
        "# MAX_SENTENCE_LENGTH = 256\n",
        "# MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "# MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "# LOW_FREQ_TOKEN = False\n",
        "# CLS = True\n",
        "# PAD_FRONT = False\n",
        "# W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "# cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "# cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "# train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "#                                                               cropped_val_data, \\\n",
        "#                                                                 cropped_test_data, \\\n",
        "#                                                                     cropped_future_data, \\\n",
        "#                                                                         MAX_SENTENCE_LENGTH, \\\n",
        "#                                                                             raw_token_pytorch_map, \\\n",
        "#                                                                                 CLS=CLS, \\\n",
        "#                                                                                     low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "#                                                                                         pad_front=PAD_FRONT)\n",
        "# pos_prior, neg_prior = get_distribution(train_y)\n",
        "# print('class prior:', pos_prior, neg_prior)\n",
        "# pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "# print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "# dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "# print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "# dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "# print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "# # pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "# print('---')\n",
        "\n",
        "# class BERT_DANN_config:\n",
        "#     # ----------------- architectual hyperparameters ----------------- #\n",
        "#     d_model = 256\n",
        "#     d_ff = 1024 # = 4* d_model\n",
        "#     n_heads = 8\n",
        "#     dropout = 0.1\n",
        "#     e_layers = 6 # actually 8 was better\n",
        "#     embedding_aggregation = 'cls' # TODO\n",
        "#     n_mlp_clf_layers = 1\n",
        "#     n_mlp_dom_layers = 1\n",
        "#     res_learning = False\n",
        "#     activation = nn.ReLU()\n",
        "#     mask_flag = False # causal mask\n",
        "#     train_embedding = True\n",
        "#     # ----------------- optimisation hyperparameters ----------------- #\n",
        "#     random_state = SEED\n",
        "#     batch_size = 8\n",
        "#     epochs = 32\n",
        "#     lr = 1e-5\n",
        "#     patience = 10\n",
        "#     # loss = nn.BCELoss()\n",
        "#     loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "#     # validation_loss = nn.BCELoss()\n",
        "#     validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "#     domain_loss = nn.BCELoss()\n",
        "#     pretrain_loss = nn.CrossEntropyLoss()\n",
        "#     pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "#     alpha = 0.1\n",
        "#     gradient_reversal_every_n_epoch = 1\n",
        "#     regularisation_loss = None\n",
        "#     scheduler = False\n",
        "#     grad_clip = False\n",
        "#     # ----------------- operation hyperparameters ----------------- #\n",
        "#     d_output = 2\n",
        "#     seq_len = MAX_SENTENCE_LENGTH\n",
        "#     n_unique_tokens = len(raw_token_pytorch_map)\n",
        "#     # ----------------- saving hyperparameters ----------------- #\n",
        "#     rootpath = home_directory + './'\n",
        "#     saving_address = home_directory + f'./results/'\n",
        "#     name = f'BERT_DANN'\n",
        "\n",
        "# model = BERT_DANN(BERT_DANN_config) # initialise the model\n",
        "\n",
        "# # train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "# best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "# print()\n",
        "\n",
        "# # as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "# model.load()\n",
        "# model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "# model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # BERT - with low freq token - SoftmaxBCELoss+OptBalAccu\n",
        "\n",
        "# MAX_SENTENCE_LENGTH = 256\n",
        "# MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "# MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "# LOW_FREQ_TOKEN = True\n",
        "# CLS = True\n",
        "# PAD_FRONT = False\n",
        "# W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "# cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "# cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "# raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "# train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "#                                                               cropped_val_data, \\\n",
        "#                                                                 cropped_test_data, \\\n",
        "#                                                                     cropped_future_data, \\\n",
        "#                                                                         MAX_SENTENCE_LENGTH, \\\n",
        "#                                                                             raw_token_pytorch_map, \\\n",
        "#                                                                                 CLS=CLS, \\\n",
        "#                                                                                     low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "#                                                                                         pad_front=PAD_FRONT)\n",
        "# pos_prior, neg_prior = get_distribution(train_y)\n",
        "# print('class prior:', pos_prior, neg_prior)\n",
        "# pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "# print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "# dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "# print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "# dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "# print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "# # pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "# print('---')\n",
        "\n",
        "# class BERT_DANN_config:\n",
        "#     # ----------------- architectual hyperparameters ----------------- #\n",
        "#     d_model = 256\n",
        "#     d_ff = 1024 # = 4* d_model\n",
        "#     n_heads = 8\n",
        "#     dropout = 0.1\n",
        "#     e_layers = 6\n",
        "#     embedding_aggregation = 'cls' # TODO\n",
        "#     n_mlp_clf_layers = 1\n",
        "#     n_mlp_dom_layers = 1\n",
        "#     res_learning = False\n",
        "#     activation = nn.ReLU()\n",
        "#     mask_flag = False # causal mask\n",
        "#     train_embedding = True\n",
        "#     # ----------------- optimisation hyperparameters ----------------- #\n",
        "#     random_state = SEED\n",
        "#     batch_size = 8\n",
        "#     epochs = 32\n",
        "#     lr = 1e-5\n",
        "#     patience = 10\n",
        "#     # loss = nn.BCELoss()\n",
        "#     loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "#     # validation_loss = nn.BCELoss()\n",
        "#     validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "#     domain_loss = nn.BCELoss()\n",
        "#     pretrain_loss = nn.CrossEntropyLoss()\n",
        "#     pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "#     alpha = 0.1\n",
        "#     gradient_reversal_every_n_epoch = 1\n",
        "#     regularisation_loss = None\n",
        "#     scheduler = False\n",
        "#     grad_clip = False\n",
        "#     # ----------------- operation hyperparameters ----------------- #\n",
        "#     d_output = 2\n",
        "#     seq_len = MAX_SENTENCE_LENGTH\n",
        "#     n_unique_tokens = len(raw_token_pytorch_map)\n",
        "#     # ----------------- saving hyperparameters ----------------- #\n",
        "#     rootpath = home_directory + './'\n",
        "#     saving_address = home_directory + f'./results/'\n",
        "#     name = f'BERT_DANN'\n",
        "\n",
        "# model = BERT_DANN(BERT_DANN_config) # initialise the model\n",
        "\n",
        "# # train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "# best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "# print()\n",
        "\n",
        "# # as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "# model.load()\n",
        "# model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "# model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "# LSTM Graveyard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM\n",
        "MAX_SENTENCE_LENGTH = 64\n",
        "MIN_FREQUENCY = 0 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = False\n",
        "PAD_FRONT = True\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "# pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class LSTM_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    n_recurrent_layers = 2\n",
        "    bidirectional = False\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    n_mlp_layers = 1\n",
        "    flatten = False\n",
        "    activation = nn.ReLU()\n",
        "    res_learning = True\n",
        "    mask_flag = False # TODO\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 4\n",
        "    loss = nn.BCELoss()\n",
        "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    validation_loss = nn.BCELoss()\n",
        "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    regularisation_loss = None\n",
        "    scheduler = True\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory +  f'./results/'\n",
        "    name = f'LSTM_Classifier'\n",
        "\n",
        "\n",
        "\n",
        "model = LSTM(LSTM_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
