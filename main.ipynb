{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Training UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect whether this is a google environment\n",
    "\n",
    "COLAB_ENVIRONMENT = False\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    COLAB_ENVIRONMENT = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "if COLAB_ENVIRONMENT:\n",
    "    py_file_location = \"./drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch\" # my private packages are stored here\n",
    "    home_directory = './drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/' # my home directory is stored in ./LAB of google drive\n",
    "    !pip install einops\n",
    "else:\n",
    "    py_file_location = './PrivatePackages/pytorch'\n",
    "    home_directory = './'\n",
    "\n",
    "sys.path.append(os.path.abspath(py_file_location))\n",
    "\n",
    "from environment import *\n",
    "from utils import *\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_class import LSTM, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "with open(home_directory + '/data/raw/comp90051-2024s1-project-1/domain1_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data1.append(json.loads(line))\n",
    "\n",
    "data2 = []\n",
    "with open(home_directory + './data/raw/comp90051-2024s1-project-1/domain2_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data2.append(json.loads(line))\n",
    "\n",
    "# create domain labels for data\n",
    "for i in range(len(data1)):\n",
    "    data1[i]['domain'] = 1\n",
    "for i in range(len(data2)):\n",
    "    data2[i]['domain'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val Test Split\n",
    "\n",
    "# get labels for stratification\n",
    "label1 = [instance['label'] for instance in data1]\n",
    "label2 = [instance['label'] for instance in data2]\n",
    "\n",
    "train_ix_1, val_test_ix_1 = train_test_split(range(len(data1)), test_size=0.3, random_state=SEED, stratify = label1)\n",
    "train_ix_2, val_test_ix_2 = train_test_split(range(len(data2)), test_size=0.3, random_state=SEED, stratify = label2)\n",
    "val_ix_1, test_ix_1 = train_test_split(val_test_ix_1, test_size=0.5, random_state=SEED, stratify = [data1[i]['label'] for i in val_test_ix_1])\n",
    "val_ix_2, test_ix_2 = train_test_split(val_test_ix_2, test_size=0.5, random_state=SEED, stratify = [data2[i]['label'] for i in val_test_ix_2])\n",
    "\n",
    "# split data according to the index from train_test_split\n",
    "train_data_1 = [data1[i] for i in train_ix_1]\n",
    "val_data_1 = [data1[i] for i in val_ix_1]\n",
    "test_data_1 = [data1[i] for i in test_ix_1]\n",
    "train_data_2 = [data2[i] for i in train_ix_2]\n",
    "val_data_2 = [data2[i] for i in val_ix_2]\n",
    "test_data_2 = [data2[i] for i in test_ix_2]\n",
    "\n",
    "# combine the data\n",
    "train_data = train_data_1 + train_data_2\n",
    "val_data = val_data_1 + val_data_2\n",
    "test_data = test_data_1 + test_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_sentence_length(data: list, max_sentence_length: int, make_cropped_remains_into_new_instance: bool) -> dict:\n",
    "    \n",
    "    \"\"\" For every sentence, reduce the length of sentence to a fixed value \n",
    "    if it is over the max_length \"\"\"\n",
    "\n",
    "    new_data = []\n",
    "\n",
    "    for instance in tqdm(data):\n",
    "        for i in range(len(data)//max_sentence_length+1):\n",
    "\n",
    "            new_instance = {}\n",
    "            \n",
    "            cropped_text = instance['text'][i*max_sentence_length:(i+1)*max_sentence_length]\n",
    "\n",
    "            if len(cropped_text) == 0:\n",
    "                continue\n",
    "\n",
    "            if i == 0 or make_cropped_remains_into_new_instance:\n",
    "\n",
    "                new_instance['text'] = cropped_text\n",
    "                new_instance['label'] = instance['label'] \n",
    "                new_instance['domain'] = instance['domain']\n",
    "                new_instance['id'] = instance['id'] + (i*0.01) # TODO\n",
    "                \n",
    "                new_instance['remains'] = 0 if i == 0 else 1 \n",
    "                \n",
    "                new_data.append(new_instance)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_token_pytorch_map(data: list, min_frequency: int = 0) -> dict:\n",
    "\n",
    "    \"\"\" get a mapping that:\n",
    "        1. places restrictions on how many times an instance is observed to be eligible to be learnt in model as an embedding\n",
    "        2. get a map of raw token values to compressed and sorted token values, with addition of CLS, Padding and Unknown.\n",
    "    \"\"\"\n",
    "    \n",
    "    tally_of_unique_train_tokens = dd(int)\n",
    "    for instance in tqdm(data):\n",
    "        for token in instance['text']:\n",
    "            tally_of_unique_train_tokens[token] += 1\n",
    "    \n",
    "    if 1 not in tally_of_unique_train_tokens: # ensure that 1 is in there so not to mess up our mapping (1 is this dataset's unknown)\n",
    "        tally_of_unique_train_tokens[1] = np.inf\n",
    "\n",
    "    list_of_unique_train_tokens = list(tally_of_unique_train_tokens.items())\n",
    "    list_of_unique_train_tokens = [token_count[0] for token_count in list_of_unique_train_tokens if token_count[1] > min_frequency]\n",
    "    list_of_unique_train_tokens.sort() # so to keep our original tokens in order\n",
    "\n",
    "    # Need to compress the used tokens (in training) onto a denser map for pytorch embedding\n",
    "    raw_token_pytorch_map = {token: i+2 for i, token in enumerate(list_of_unique_train_tokens)}\n",
    "    raw_token_pytorch_map['CLS'] = 0 # CLS takes on 0 in our map\n",
    "    raw_token_pytorch_map['PAD'] = 1 # Padding takes on 1 in our map\n",
    "    raw_token_pytorch_map['UNK'] = 2 # Unknown takes on 2 in our map as per data (in data 1 was value of unknown)\n",
    "\n",
    "    # even if we don't decide to use CLS, it doesn't affect our model at all.\n",
    "\n",
    "    return raw_token_pytorch_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReTokenise_Tokens(data, raw_token_pytorch_map, max_sentence_length, CLS=True):\n",
    "    \n",
    "    \"\"\" Convert the Token index into one useable by Pytorch Embedding Layer \"\"\"\n",
    "    \n",
    "    data = copy.deepcopy(data)\n",
    "    for instance in tqdm(data):\n",
    "        instance['text'] = instance['text'] = [raw_token_pytorch_map[token] if token in raw_token_pytorch_map else raw_token_pytorch_map['UNK'] for token in instance['text']]\n",
    "        if CLS: \n",
    "            instance['text'] = [raw_token_pytorch_map['CLS']] + instance['text']\n",
    "\n",
    "    if max_sentence_length:\n",
    "        for instance in data:\n",
    "            if len(instance['text']) < max_sentence_length:\n",
    "                instance['text'] = instance['text'] + [raw_token_pytorch_map['PAD']] * (max_sentence_length - len(instance['text'])) # 1 is pad in our map\n",
    "            else:\n",
    "                instance['text'] = instance['text'][:max_sentence_length]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Factory(train_data, val_data, test_data, max_sentence_length, raw_token_pytorch_map, CLS=True):\n",
    "    \n",
    "    \"\"\" Convert our (cropped) data into train x, train y, val x, val y, test x, test y etc \"\"\"\n",
    "\n",
    "    train_data_transformed = ReTokenise_Tokens(train_data, raw_token_pytorch_map, max_sentence_length, CLS)\n",
    "    val_data_transformed = ReTokenise_Tokens(val_data, raw_token_pytorch_map, max_sentence_length, CLS)\n",
    "    test_data_transformed = ReTokenise_Tokens(test_data, raw_token_pytorch_map, max_sentence_length, CLS)\n",
    "\n",
    "    train_x = [instance['text'] for instance in train_data_transformed]\n",
    "    train_y = [instance['label'] for instance in train_data_transformed]\n",
    "    val_x = [instance['text'] for instance in val_data_transformed]\n",
    "    val_y = [instance['label'] for instance in val_data_transformed]\n",
    "    test_x = [instance['text'] for instance in test_data_transformed]\n",
    "    test_y = [instance['label'] for instance in test_data_transformed]\n",
    "\n",
    "    train_y = [[0, 1] if label == 1 else [1, 0] for label in train_y] #TODO: check this\n",
    "    val_y = [[0, 1] if label == 1 else [1, 0] for label in val_y]\n",
    "    test_y = [[0, 1] if label == 1 else [1, 0] for label in test_y]\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(train_y) -> Tuple[float, float]:\n",
    "\n",
    "    \"\"\" get the distribution of labels in this set - for processing the loss function \"\"\"\n",
    "\n",
    "    label = [y[1] for y in train_y]\n",
    "\n",
    "    return np.mean(label), 1-np.mean(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_DataFactory(data: list, context_window: int, seed: int, raw_token_pytorch_map: dict, k) -> list:\n",
    "\n",
    "    \"\"\" Get W2V training data \"\"\"\n",
    "    \n",
    "    assert context_window % 2 == 1, 'context window must be odd'\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    MAX_SAMPLED_NEGATIVE_TOKENS = 10000\n",
    "\n",
    "    retokenised_keys = list(raw_token_pytorch_map.keys())\n",
    "\n",
    "    negative_tokens = np.random.choice(retokenised_keys, MAX_SAMPLED_NEGATIVE_TOKENS)\n",
    "\n",
    "    negative_up_to = 0\n",
    "\n",
    "    w2v_data = []\n",
    "\n",
    "    for instance in tqdm(data):\n",
    "        tokens = [context_window//2 * 'CLS'] + instance['text'] + [context_window//2 * raw_token_pytorch_map['PAD']]\n",
    "\n",
    "        for i in range(context_window//2, len(tokens) - context_window//2):\n",
    "            \n",
    "            focus_token_retokenised = raw_token_pytorch_map.get(tokens[i], raw_token_pytorch_map['UNK'])\n",
    "            context_words = set()\n",
    "\n",
    "            for j in range(-context_window//2, context_window//2+1):\n",
    "                if j != 0: # don't want to make positive sample with self\n",
    "                    if tokens[j] in context_words: # CLS and Padding (being start and end) being repeated\n",
    "                        continue \n",
    "                    \n",
    "                    new_instance = {'token': focus_token_retokenised, 'context': raw_token_pytorch_map.get(tokens[j], raw_token_pytorch_map['UNK']), 'label': 1}\n",
    "                    w2v_data.append(new_instance)\n",
    "                    context_words.add(tokens[j])\n",
    "            \n",
    "            for j in range(len(context_words)): # sample the same number of negatives\n",
    "                # TODO: different for each round?\n",
    "                while True:\n",
    "                    \n",
    "                    if negative_up_to == MAX_SAMPLED_NEGATIVE_TOKENS:\n",
    "                        negative_up_to = 0\n",
    "                        #TODO: shuffle\n",
    "\n",
    "                    sampled_negative_retokenised = negative_tokens[negative_up_to]\n",
    "                    negative_up_to += 1\n",
    "                    if sampled_negative_retokenised not in context_words: # didn't sample a positive case\n",
    "                        break\n",
    "\n",
    "                new_instance = {'token': focus_token_retokenised, 'context': sampled_negative_retokenised, 'label': 0}\n",
    "                w2v_data.append(new_instance)\n",
    "    \n",
    "    return w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 128\n",
    "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
    "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
    "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 3766/12600 [00:00<00:00, 37654.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [00:00<00:00, 26908.93it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 94999.63it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 120369.69it/s]\n"
     ]
    }
   ],
   "source": [
    "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance =MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
    "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
    "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance =MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [00:00<00:00, 48064.47it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [00:00<00:00, 70358.86it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 60702.95it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 55708.92it/s]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y, test_x, test_y = Data_Factory(cropped_train_data, \\\n",
    "                                                              cropped_val_data, \\\n",
    "                                                                cropped_test_data, \\\n",
    "                                                                    MAX_SENTENCE_LENGTH, \\\n",
    "                                                                        raw_token_pytorch_map, \\\n",
    "                                                                            CLS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_prior, neg_prior = get_distribution(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "W2V_DataFactory() missing 1 required positional argument: 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_w2v_data \u001b[38;5;241m=\u001b[39m \u001b[43mW2V_DataFactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW2V_CONTEXT_WINDOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_token_pytorch_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mraw_token_pytorch_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m val_w2v_data \u001b[38;5;241m=\u001b[39m W2V_DataFactory(val_data, context_window\u001b[38;5;241m=\u001b[39m W2V_CONTEXT_WINDOW, seed \u001b[38;5;241m=\u001b[39m SEED, raw_token_pytorch_map \u001b[38;5;241m=\u001b[39m raw_token_pytorch_map)\n",
      "\u001b[0;31mTypeError\u001b[0m: W2V_DataFactory() missing 1 required positional argument: 'k'"
     ]
    }
   ],
   "source": [
    "train_w2v_data = W2V_DataFactory(train_data, context_window= W2V_CONTEXT_WINDOW, seed = SEED, raw_token_pytorch_map = raw_token_pytorch_map)\n",
    "val_w2v_data = W2V_DataFactory(val_data, context_window= W2V_CONTEXT_WINDOW, seed = SEED, raw_token_pytorch_map = raw_token_pytorch_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset():\n",
    "#     \"\"\" Pytorch style dataset \"\"\"\n",
    "\n",
    "#     def __init__(self, data, maxlen):\n",
    "#         self.data = data\n",
    "#         self.maxlen = maxlen\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data[index]['text'], self.data[index]['label']\n",
    "#         # return self.data[index]['text'], self.data[index]['label'], self.data[index]['domain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Models\n",
    "\n",
    "#### 1. Prediction only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/99 [00:00<?, ?it/s]/Users/tg.chenny/Desktop/1. University/2. Masters/7. Statistical Machine Learning/Assignments/A1/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch/model/model_class/__template__.py:101: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.model.parameters(), 2)\n",
      "  3%|▎         | 3/99 [00:22<11:54,  7.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTM(LSTM_config) \u001b[38;5;66;03m# initialise the model\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# train the model (all cells except this one will print training log and evaluation at each batch)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/1. University/2. Masters/7. Statistical Machine Learning/Assignments/A1/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch/model/model_class/__template__.py:99\u001b[0m, in \u001b[0;36mClassificationModel.fit\u001b[0;34m(self, total_train_X, total_train_y, total_val_test_X, total_val_test_y)\u001b[0m\n\u001b[1;32m     96\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(pred, true)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mgrad_clip: \u001b[38;5;66;03m# gradient clip\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "class LSTM_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 256\n",
    "    n_recurrent_layers = 1\n",
    "    bidirectional = True\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    n_mlp_layers = 0\n",
    "    flatten = False\n",
    "    activation = nn.ReLU()\n",
    "    res_learning = False\n",
    "    mask_flag = False # TODO\n",
    "    train_embedding = False\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 128\n",
    "    epochs = 32\n",
    "    lr = 1e-3\n",
    "    patience = 5\n",
    "    # loss = nn.BCELoss()\n",
    "    loss = WeightedBinaryCrossEntropyLoss(1-pos_prior, 1-neg_prior)\n",
    "    # validation_loss = nn.BCELoss()\n",
    "    validation_loss = WeightedBinaryCrossEntropyLoss(1-pos_prior, 1-neg_prior)\n",
    "    regularisation_loss = None\n",
    "    scheduler = True\n",
    "    grad_clip = True\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = MAX_SENTENCE_LENGTH\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory + './'\n",
    "    saving_address = home_directory +  f'./results/'\n",
    "    name = f'LSTM_Classifier'\n",
    "    \n",
    "\n",
    "\n",
    "model = LSTM(LSTM_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_y, val_x, val_y)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_y, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 98/99 [03:31<00:02,  2.99s/it]"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "\n",
    "class BERT_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 128\n",
    "    d_ff = 512 # = 4* d_model\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    e_layers = 3 \n",
    "    embedding_aggregation = 'cls' # TODO\n",
    "    n_mlp_layers = 0\n",
    "    res_learning = False\n",
    "    activation = nn.ReLU()\n",
    "    mask_flag = False # causal mask\n",
    "    train_embedding = False\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 128\n",
    "    epochs = 2\n",
    "    lr = 1e-3\n",
    "    patience = 2\n",
    "    loss = nn.BCELoss()\n",
    "    validation_loss = nn.BCELoss()\n",
    "    regularisation_loss = None\n",
    "    scheduler = False\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = MAX_SENTENCE_LENGTH\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory + './'\n",
    "    saving_address = home_directory + f'./results/'\n",
    "    name = f'BERT_Classifier'\n",
    "    \n",
    "\n",
    "\n",
    "model = BERT(BERT_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_y, val_x, val_y)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_y, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGramLoss(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramLoss, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Initialize the weights of the embedding layer\n",
    "        self.embeddings.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_word_indices, context_word_indices):\n",
    "        # Lookup embeddings for input word indices\n",
    "        input_embeddings = self.embeddings(input_word_indices)\n",
    "        \n",
    "        # Predict context word logits\n",
    "        output_logits = self.output_layer(input_embeddings)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = nn.CrossEntropyLoss()(output_logits, context_word_indices)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Domain Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP90051",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
