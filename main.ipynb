{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect whether this is a google environment\n",
    "\n",
    "COLAB_ENVIRONMENT = False\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    COLAB_ENVIRONMENT = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "if COLAB_ENVIRONMENT:\n",
    "    py_file_location = \"./drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch\" # my private packages are stored here\n",
    "    home_directory = './drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/' # my home directory is stored in ./LAB of google drive\n",
    "    !pip install einops\n",
    "else:\n",
    "    py_file_location = './PrivatePackages/pytorch'\n",
    "    home_directory = './'\n",
    "\n",
    "sys.path.append(os.path.abspath(py_file_location))\n",
    "\n",
    "from environment import *\n",
    "from utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_class import LSTM, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "with open('./data/raw/comp90051-2024s1-project-1/domain1_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data1.append(json.loads(line))\n",
    "\n",
    "data2 = []\n",
    "with open('./data/raw/comp90051-2024s1-project-1/domain2_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data2.append(json.loads(line))\n",
    "\n",
    "# create domain labels for data\n",
    "for i in range(len(data1)):\n",
    "    data1[i]['domain'] = 1\n",
    "for i in range(len(data2)):\n",
    "    data2[i]['domain'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val Test Split\n",
    "\n",
    "# get labels for stratification\n",
    "label1 = [instance['label'] for instance in data1]\n",
    "label2 = [instance['label'] for instance in data2]\n",
    "\n",
    "train_ix_1, val_test_ix_1 = train_test_split(range(len(data1)), test_size=0.3, random_state=SEED, stratify = label1)\n",
    "train_ix_2, val_test_ix_2 = train_test_split(range(len(data2)), test_size=0.3, random_state=SEED, stratify = label2)\n",
    "val_ix_1, test_ix_1 = train_test_split(val_test_ix_1, test_size=0.5, random_state=SEED, stratify = [data1[i]['label'] for i in val_test_ix_1])\n",
    "val_ix_2, test_ix_2 = train_test_split(val_test_ix_2, test_size=0.5, random_state=SEED, stratify = [data2[i]['label'] for i in val_test_ix_2])\n",
    "\n",
    "# split data according to the index from train_test_split\n",
    "train_data_1 = [data1[i] for i in train_ix_1]\n",
    "val_data_1 = [data1[i] for i in val_ix_1]\n",
    "test_data_1 = [data1[i] for i in test_ix_1]\n",
    "train_data_2 = [data2[i] for i in train_ix_2]\n",
    "val_data_2 = [data2[i] for i in val_ix_2]\n",
    "test_data_2 = [data2[i] for i in test_ix_2]\n",
    "\n",
    "# combine the data\n",
    "train_data = train_data_1 + train_data_2\n",
    "val_data = val_data_1 + val_data_2\n",
    "test_data = test_data_1 + test_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_unique_train_tokens = set()\n",
    "for instance in train_data:\n",
    "    set_of_unique_train_tokens.update(instance['text'][:64])\n",
    "\n",
    "list_of_unique_train_tokens = list(set_of_unique_train_tokens)\n",
    "list_of_unique_train_tokens.sort()\n",
    "\n",
    "# Need to compress the used tokens (in training) onto a denser map for pytorch embedding\n",
    "raw_token_pytorch_map = {token: i+2 for i, token in enumerate(list_of_unique_train_tokens)}\n",
    "raw_token_pytorch_map['CLS'] = 0 # CLS takes on 0 in our map\n",
    "raw_token_pytorch_map['PAD'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = dict()\n",
    "word_freq_1 = dict()\n",
    "word_freq_2 = dict()\n",
    "for i in range(len(data1)):\n",
    "    for word in data1[i]['text']:\n",
    "        if word not in word_freq_1:\n",
    "            word_freq_1[word] = 1\n",
    "        else:\n",
    "            word_freq_1[word] += 1\n",
    "\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] += 1\n",
    "\n",
    "for i in range(len(data2)):\n",
    "    for word in data2[i]['text']:\n",
    "        if word not in word_freq_2:\n",
    "            word_freq_2[word] = 1\n",
    "        else:\n",
    "            word_freq_2[word] += 1\n",
    "        \n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] += 1\n",
    "\n",
    "word_freq_1 = sorted(word_freq_1.items(), key=lambda x: x[1], reverse=True)\n",
    "word_freq_2 = sorted(word_freq_2.items(), key=lambda x: x[1], reverse=True)\n",
    "word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "word_freq = [x[0] for x in word_freq if x[1] > 100]\n",
    "word_freq.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataFactory(data, raw_token_pytorch_map, max_len, CLS=True):\n",
    "    \"\"\" Convert the Token index into one useable by Pytorch Embedding Layer \"\"\"\n",
    "    data = copy.deepcopy(data)\n",
    "    for instance in tqdm(data):\n",
    "        instance['text'] = instance['text'] = [raw_token_pytorch_map[token] if token in raw_token_pytorch_map else 2 for token in instance['text']]\n",
    "        # because 2 is our default value\n",
    "        if CLS: # CLS takes on 0 in our map\n",
    "            instance['text'] = [raw_token_pytorch_map['CLS']] + instance['text']\n",
    "\n",
    "    if max_len:\n",
    "        for instance in data:\n",
    "            if len(instance['text']) < max_len:\n",
    "                instance['text'] = instance['text'] + [raw_token_pytorch_map['PAD']] * (max_len - len(instance['text'])) # 1 is pad in our map\n",
    "            else:\n",
    "                instance['text'] = instance['text'][:max_len]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [00:00<00:00, 23884.62it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 27390.08it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 30750.03it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_transformed = DataFactory(train_data, raw_token_pytorch_map, 64)\n",
    "train_data_transformed = train_data_transformed[:200]\n",
    "val_data_transformed = DataFactory(val_data, raw_token_pytorch_map, 64)\n",
    "val_data_transformed = val_data_transformed[:200]\n",
    "test_data_transformed = DataFactory(test_data, raw_token_pytorch_map, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [instance['text'] for instance in train_data_transformed]\n",
    "train_y = [instance['label'] for instance in train_data_transformed]\n",
    "val_x = [instance['text'] for instance in val_data_transformed]\n",
    "val_y = [instance['label'] for instance in val_data_transformed]\n",
    "test_x = [instance['text'] for instance in test_data_transformed]\n",
    "test_y = [instance['label'] for instance in test_data_transformed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [[0, 1] if label == 1 else [1, 0] for label in train_y]\n",
    "val_y = [[0, 1] if label == 1 else [1, 0] for label in val_y]\n",
    "test_y = [[0, 1] if label == 1 else [1, 0] for label in test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset():\n",
    "#     \"\"\" Pytorch style dataset \"\"\"\n",
    "\n",
    "#     def __init__(self, data, maxlen):\n",
    "#         self.data = data\n",
    "#         self.maxlen = maxlen\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data[index]['text'], self.data[index]['label']\n",
    "#         # return self.data[index]['text'], self.data[index]['label'], self.data[index]['domain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Models\n",
    "\n",
    "#### 1. Prediction only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]/Users/tg.chenny/Desktop/1. University/2. Masters/7. Statistical Machine Learning/Assignments/A1/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch/model/model_class/__template__.py:101: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.model.parameters(), 2)\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train | Loss:  0.6919 | Accuracy:  0.5050| F1:  0.5520 | Balanced Accuracy:  0.5005 \n",
      "Epoch 1 Val   | Loss:  0.6878 | Accuracy:  0.5750| F1:  0.5933 | Balanced Accuracy:  0.5761 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/Users/tg.chenny/Desktop/1. University/2. Masters/7. Statistical Machine Learning/Assignments/A1/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch/model/model_class/__template__.py:101: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.model.parameters(), 2)\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train | Loss:  0.6818 | Accuracy:  0.6700| F1:  0.6857 | Balanced Accuracy:  0.6694 \n",
      "Epoch 2 Val   | Loss:  0.6837 | Accuracy:  0.5550| F1:  0.5782 | Balanced Accuracy:  0.5563 \n",
      "\n",
      "Epoch 2 Val   | Loss:  0.6837 | Accuracy:  0.5550| F1:  0.5782 | Balanced Accuracy:  0.5563 \n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "class LSTM_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 128\n",
    "    n_recurrent_layers = 1\n",
    "    bidirectional = True\n",
    "    n_heads = 0\n",
    "    dropout = 0.1\n",
    "    n_mlp_layers = 0\n",
    "    flatten = False\n",
    "    activation = nn.ReLU()\n",
    "    res_learning = False\n",
    "    mask_flag = False\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 128\n",
    "    epochs = 2\n",
    "    lr = 1e-3\n",
    "    patience = 5\n",
    "    loss = nn.BCELoss()\n",
    "    validation_loss = nn.BCELoss()\n",
    "    regularisation_loss = None\n",
    "    scheduler = False\n",
    "    grad_clip = True\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = 64\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = './'\n",
    "    saving_address = f'./results/'\n",
    "    name = f'LSTM_Classifier'\n",
    "    \n",
    "\n",
    "\n",
    "model = LSTM(LSTM_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_y, val_x, val_y)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_y, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4894677698612213, 0.5105322599411011],\n",
       " [0.6054280996322632, 0.3945719301700592],\n",
       " [0.4914986193180084, 0.508501410484314],\n",
       " [0.48853105306625366, 0.5114689469337463],\n",
       " [0.49079567193984985, 0.5092043280601501],\n",
       " [0.49549156427383423, 0.504508376121521],\n",
       " [0.4960257112979889, 0.5039742588996887],\n",
       " [0.4931079149246216, 0.5068920850753784],\n",
       " [0.48357969522476196, 0.5164202451705933],\n",
       " [0.47911831736564636, 0.520881712436676]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_x)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5132982730865479, 0.4867016673088074],\n",
       " [0.6065260767936707, 0.39347392320632935],\n",
       " [0.4989036023616791, 0.5010964274406433],\n",
       " [0.4935510754585266, 0.5064489841461182],\n",
       " [0.4965856969356537, 0.5034143924713135],\n",
       " [0.4941680133342743, 0.5058320164680481],\n",
       " [0.4849659204483032, 0.5150341391563416],\n",
       " [0.5000945925712585, 0.49990543723106384],\n",
       " [0.5082995295524597, 0.49170055985450745],\n",
       " [0.4829690158367157, 0.5170309543609619]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(val_x)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train | Loss:  1.4933 | Accuracy:  0.4900| F1:  0.5641 | Balanced Accuracy:  0.4815 \n",
      "Epoch 1 Val   | Loss:  1.2334 | Accuracy:  0.5100| F1:  0.0000 | Balanced Accuracy:  0.5000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train | Loss:  0.9996 | Accuracy:  0.4700| F1:  0.0000 | Balanced Accuracy:  0.5000 \n",
      "Epoch 2 Val   | Loss:  0.8670 | Accuracy:  0.4900| F1:  0.6577 | Balanced Accuracy:  0.5000 \n",
      "\n",
      "Epoch 2 Val   | Loss:  0.8670 | Accuracy:  0.4900| F1:  0.6577 | Balanced Accuracy:  0.5000 \n"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "\n",
    "class BERT_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 128\n",
    "    d_ff = 512\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    e_layers = 3\n",
    "    embedding_aggregation = 'cls'\n",
    "    n_mlp_layers = 0\n",
    "    res_learning = False\n",
    "    activation = nn.ReLU()\n",
    "    mask_flag = False # causal mask\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 128\n",
    "    epochs = 2\n",
    "    lr = 1e-3\n",
    "    patience = 2\n",
    "    loss = nn.BCELoss()\n",
    "    validation_loss = nn.BCELoss()\n",
    "    regularisation_loss = None\n",
    "    scheduler = False\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = 64\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = './'\n",
    "    saving_address = f'./results/'\n",
    "    name = f'BERT_Classifier'\n",
    "    \n",
    "\n",
    "\n",
    "model = BERT(BERT_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_y, val_x, val_y)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_y, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.23434628546237946, 0.7656537294387817],\n",
       " [0.24878013134002686, 0.7512198686599731],\n",
       " [0.2295651137828827, 0.7704349160194397],\n",
       " [0.2324276715517044, 0.7675723433494568],\n",
       " [0.23661142587661743, 0.7633885145187378],\n",
       " [0.23522336781024933, 0.7647765874862671],\n",
       " [0.2274104803800583, 0.7725894451141357],\n",
       " [0.22060248255729675, 0.7793975472450256],\n",
       " [0.24059750139713287, 0.7594025135040283],\n",
       " [0.2309846580028534, 0.769015371799469]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_x)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.22902269661426544, 0.7709773182868958],\n",
       " [0.2724650204181671, 0.7275350093841553],\n",
       " [0.22975383698940277, 0.7702462077140808],\n",
       " [0.23741821944713593, 0.7625817060470581],\n",
       " [0.2260596752166748, 0.77394038438797],\n",
       " [0.24640795588493347, 0.7535920143127441],\n",
       " [0.232538640499115, 0.7674614191055298],\n",
       " [0.22662198543548584, 0.7733779549598694],\n",
       " [0.23393823206424713, 0.7660617828369141],\n",
       " [0.23056721687316895, 0.7694327235221863]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(val_x)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGramLoss(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramLoss, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Initialize the weights of the embedding layer\n",
    "        self.embeddings.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_word_indices, context_word_indices):\n",
    "        # Lookup embeddings for input word indices\n",
    "        input_embeddings = self.embeddings(input_word_indices)\n",
    "        \n",
    "        # Predict context word logits\n",
    "        output_logits = self.output_layer(input_embeddings)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = nn.CrossEntropyLoss()(output_logits, context_word_indices)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Domain Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP90051",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
