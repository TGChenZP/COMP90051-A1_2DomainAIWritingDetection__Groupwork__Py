{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYRTCgGLL-s-"
      },
      "source": [
        "# Pytorch Training UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaPEVpXNL-tC"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCsumf5GL-tC",
        "outputId": "0ba3ab0e-1680-43cc-cfce-bb544f115c77"
      },
      "outputs": [],
      "source": [
        "# detect whether this is a google environment\n",
        "\n",
        "COLAB_ENVIRONMENT = False\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    COLAB_ENVIRONMENT = True\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5xuQPsyL-tD",
        "outputId": "9f22e09e-d9ee-4be3-823f-b1e746c23356"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "if COLAB_ENVIRONMENT:\n",
        "    py_file_location = \"./drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch\" # my private packages are stored here\n",
        "    home_directory = './drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/' # my home directory is stored in ./LAB of google drive\n",
        "    !pip install einops\n",
        "else:\n",
        "    py_file_location = './PrivatePackages/pytorch'\n",
        "    home_directory = './'\n",
        "\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "from environment import *\n",
        "from utils import *\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GkmAo_TgL-tE"
      },
      "outputs": [],
      "source": [
        "from model.model_class import LSTM, BERT, LSTM_DANN, BERT_DANN, LSTM_DCE_DANN, BERT_DCE_DANN, LSTM_Hinge, BERT_Hinge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k-bq7crL-tE"
      },
      "source": [
        "### Set Seed and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bo5HdEOrL-tF"
      },
      "outputs": [],
      "source": [
        "SEED = 2608"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9FjVsxYYL-tF"
      },
      "outputs": [],
      "source": [
        "data1 = []\n",
        "with open(home_directory + '/data/raw/comp90051-2024s1-project-1/domain1_train_data.json', 'r') as f:\n",
        "    for line in f:\n",
        "        data1.append(json.loads(line))\n",
        "\n",
        "data2 = []\n",
        "with open(home_directory + './data/raw/comp90051-2024s1-project-1/domain2_train_data.json', 'r') as f:\n",
        "    for line in f:\n",
        "        data2.append(json.loads(line))\n",
        "\n",
        "data_test = []\n",
        "with open(home_directory + '/data/raw/comp90051-2024s1-project-1/test_data.json', 'r') as f:\n",
        "    for line in f:\n",
        "        data_test.append(json.loads(line))\n",
        "\n",
        "# create domain labels for data\n",
        "for i in range(len(data1)):\n",
        "    data1[i]['domain'] = 0\n",
        "for i in range(len(data2)):\n",
        "    data2[i]['domain'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TitTH75uL-tG"
      },
      "outputs": [],
      "source": [
        "# Train Val Test Split\n",
        "\n",
        "# get labels for stratification\n",
        "label1 = [instance['label'] for instance in data1]\n",
        "label2 = [instance['label'] for instance in data2]\n",
        "\n",
        "train_ix_1, val_test_ix_1 = train_test_split(range(len(data1)), test_size=0.3, random_state=SEED, stratify = label1)\n",
        "train_ix_2, val_test_ix_2 = train_test_split(range(len(data2)), test_size=0.3, random_state=SEED, stratify = label2)\n",
        "val_ix_1, test_ix_1 = train_test_split(val_test_ix_1, test_size=0.5, random_state=SEED, stratify = [data1[i]['label'] for i in val_test_ix_1])\n",
        "val_ix_2, test_ix_2 = train_test_split(val_test_ix_2, test_size=0.5, random_state=SEED, stratify = [data2[i]['label'] for i in val_test_ix_2])\n",
        "\n",
        "# split data according to the index from train_test_split\n",
        "train_data_1 = [data1[i] for i in train_ix_1]\n",
        "val_data_1 = [data1[i] for i in val_ix_1]\n",
        "test_data_1 = [data1[i] for i in test_ix_1]\n",
        "train_data_2 = [data2[i] for i in train_ix_2]\n",
        "val_data_2 = [data2[i] for i in val_ix_2]\n",
        "test_data_2 = [data2[i] for i in test_ix_2]\n",
        "\n",
        "# combine the data\n",
        "train_data = train_data_1 + train_data_2\n",
        "val_data = val_data_1 + val_data_2\n",
        "test_data = test_data_1 + test_data_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20T2YmtZL-tI"
      },
      "source": [
        "---\n",
        "### Models\n",
        "\n",
        "#### 1. Pure Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxKOHgAtHdMS",
        "outputId": "664ec65f-1c8a-4c30-ad94-af9446c5c86a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12600/12600 [00:00<00:00, 28934.71it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 93906.22it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 120085.05it/s]\n",
            " 11%|█▏        | 453/4000 [00:00<00:00, 64558.45it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m cropped_val_data \u001b[38;5;241m=\u001b[39m crop_sentence_length(val_data, max_sentence_length \u001b[38;5;241m=\u001b[39m  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m cropped_test_data \u001b[38;5;241m=\u001b[39m crop_sentence_length(test_data, max_sentence_length \u001b[38;5;241m=\u001b[39m MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m cropped_future_data \u001b[38;5;241m=\u001b[39m \u001b[43mcrop_sentence_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sentence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAX_SENTENCE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_cropped_remains_into_new_instance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m raw_token_pytorch_map \u001b[38;5;241m=\u001b[39m get_raw_token_pytorch_map(data \u001b[38;5;241m=\u001b[39m cropped_train_data, min_frequency \u001b[38;5;241m=\u001b[39m MIN_FREQUENCY)\n\u001b[1;32m     15\u001b[0m train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x \u001b[38;5;241m=\u001b[39m Data_Factory(cropped_train_data, \\\n\u001b[1;32m     16\u001b[0m                                                               cropped_val_data, \\\n\u001b[1;32m     17\u001b[0m                                                                 cropped_test_data, \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                                                     low_freq_special_token\u001b[38;5;241m=\u001b[39mLOW_FREQ_TOKEN, \\\n\u001b[1;32m     23\u001b[0m                                                                                         pad_front\u001b[38;5;241m=\u001b[39mPAD_FRONT)\n",
            "File \u001b[0;32m~/Desktop/1. University/2. Masters/7. Statistical Machine Learning/Assignments/A1/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch/utils/data.py:15\u001b[0m, in \u001b[0;36mcrop_sentence_length\u001b[0;34m(data, max_sentence_length, make_cropped_remains_into_new_instance)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mmax_sentence_length\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     13\u001b[0m     new_instance \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 15\u001b[0m     cropped_text \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmax_sentence_length\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmax_sentence_length\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cropped_text) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# BERT - CELoss\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    # loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    validation_loss = nn.CrossEntropyLoss()\n",
        "    # validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_Classifier'\n",
        "\n",
        "model = BERT(BERT_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "# pretrain_best_epoch = model.fit_pretrain(pretrain_x, pretrain_y, pretrain_dom, pretrain_mask, preval_x, preval_y, preval_dom, preval_mask)\n",
        "# print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "# model.load()\n",
        "# model.eval_pretrain(preval_x, preval_y, preval_dom, preval_mask, pretrain_best_epoch, evaluation_mode = True)\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT - WCELoss\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_Classifier'\n",
        "\n",
        "model = BERT(BERT_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "# pretrain_best_epoch = model.fit_pretrain(pretrain_x, pretrain_y, pretrain_dom, pretrain_mask, preval_x, preval_y, preval_dom, preval_mask)\n",
        "# print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "# model.load()\n",
        "# model.eval_pretrain(preval_x, preval_y, preval_dom, preval_mask, pretrain_best_epoch, evaluation_mode = True)\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWSc_yZ8iWh0",
        "outputId": "7fc76584-fde3-4bde-fa41-f7a6054ec771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 Val   | Loss:  0.2023 | Accuracy:  0.8459| F1:  0.7007 | Balanced Accuracy:  0.8337 | Dom Avg Accuracy:  0.7931 |\n",
            "                Domain 1 Accuracy:  0.7587| Domain 1 F1:  0.7774 | Domain 1 Balanced Accuracy:  0.7587 | \n",
            "                Domain 2 Accuracy:  0.8795| Domain 2 F1:  0.5927 | Domain 2 Balanced Accuracy:  0.8275\n",
            "Epoch 20 Val   | Loss:  0.2083 | Accuracy:  0.8385| F1:  0.6845 | Balanced Accuracy:  0.8206 | Dom Avg Accuracy:  0.7759 |\n",
            "                Domain 1 Accuracy:  0.7320| Domain 1 F1:  0.7528 | Domain 1 Balanced Accuracy:  0.7320 | \n",
            "                Domain 2 Accuracy:  0.8795| Domain 2 F1:  0.5870 | Domain 2 Balanced Accuracy:  0.8198\n"
          ]
        }
      ],
      "source": [
        "# model.load()\n",
        "# model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "# model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)\n",
        "\n",
        "# EXPERIMENT_NAME = '8bert_1mlpc_1mlpd_256d1024_256d40_8h_0.1_embed_4batch_bwce'\n",
        "\n",
        "# future_pred_y = model.predict(future_x)\n",
        "\n",
        "# future_pred_y = [1 if x[1] > x[0] else 0 for x in future_pred_y]\n",
        "\n",
        "# predictions = pd.DataFrame({'id': range(len(future_pred_y)), 'class': future_pred_y})\n",
        "# predictions.to_csv(home_directory + f'predictions/{EXPERIMENT_NAME}_classification.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVDueBf3L-tJ"
      },
      "source": [
        "---\n",
        "#### 2. DANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT - DANN CELoss\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_DANN'\n",
        "\n",
        "model = BERT_DANN(BERT_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OvktIasyOy_",
        "outputId": "be7af3fc-c82b-4978-d4f6-b73ab55b7029"
      },
      "outputs": [],
      "source": [
        "# BERT - DANN WCELoss\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_DANN'\n",
        "\n",
        "model = BERT_DANN(BERT_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srUFUxEV0jK0",
        "outputId": "e696fc9a-4a55-42d5-a9cd-9dba2af2d9ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12600/12600 [00:00<00:00, 15485.68it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 25772.33it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 3127.14it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 41878.65it/s]\n",
            "100%|██████████| 12600/12600 [00:01<00:00, 12264.92it/s]\n",
            "100%|██████████| 12600/12600 [00:00<00:00, 37174.52it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 40789.31it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 35825.73it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 39010.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class prior: 0.2222222222222222 0.7777777777777778\n",
            "domain prior: 0.7222222222222222 0.2777777777777778\n",
            "dom1 class prior: 0.5 0.5\n",
            "dom2 class prior: 0.11538461538461539 0.8846153846153846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12600/12600 [00:02<00:00, 4654.85it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 6566.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Epoch 21 Val   | Classification Loss:  0.1625 | Accuracy:  0.8574| F1:  0.7150 | Balanced Accuracy:  0.8387 | Dom Avg Accuracy:  0.8010 |\n",
            "                            Domain Loss:  0.5397 | Domain Accuracy:  0.7581 |  \n",
            "                            Domain 1 Accuracy:  0.7920| Domain 1 F1:  0.8050 | Domain 1 Balanced Accuracy:  0.7920 |  \n",
            "                            Domain 2 Accuracy:  0.8826| Domain 2 F1:  0.5844 | Domain 2 Balanced Accuracy:  0.8100\n",
            "Epoch 21 Val   | Classification Loss:  0.1658 | Accuracy:  0.8589| F1:  0.7167 | Balanced Accuracy:  0.8390 | Dom Avg Accuracy:  0.8013 |\n",
            "                            Domain Loss:  0.5434 | Domain Accuracy:  0.7607 |  \n",
            "                            Domain 1 Accuracy:  0.7667| Domain 1 F1:  0.7804 | Domain 1 Balanced Accuracy:  0.7667 |  \n",
            "                            Domain 2 Accuracy:  0.8944| Domain 2 F1:  0.6241 | Domain 2 Balanced Accuracy:  0.8359\n"
          ]
        }
      ],
      "source": [
        "# EXPERIMENT_NAME = '8bert_1mlpc_1mlpd_256d1024_256d40_8h_0.1_embed_4batch_bwce_low_freq'\n",
        "\n",
        "# future_pred_y, future_pred_dom = model.predict(future_x)\n",
        "\n",
        "# future_pred_y = [1 if x[1] > x[0] else 0 for x in future_pred_y]\n",
        "\n",
        "# predictions = pd.DataFrame({'id': range(len(future_pred_y)), 'class': future_pred_y})\n",
        "# predictions.to_csv(home_directory + f'predictions/{EXPERIMENT_NAME}_classification.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycRT4iArPckp"
      },
      "source": [
        "---\n",
        "#### 3. DCE_DANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YQC0zod66Xu",
        "outputId": "01e231cc-2549-4d60-f950-19080dbe2275"
      },
      "outputs": [],
      "source": [
        "# BERT DBCE - celoss expr - just bal domain\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DCE_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    domain_1_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom1_pos_prior, dom1_neg_prior]))\n",
        "    domain_1_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_1_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom1_pos_prior, dom1_neg_prior]))\n",
        "    domain_2_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom2_pos_prior, dom2_neg_prior]))\n",
        "    domain_2_validation_loss = nn.CrossEntropyLoss()\n",
        "    # domain_2_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom2_pos_prior, dom2_neg_prior]))\n",
        "    domain_prior = [pos_dom_prior, neg_dom_prior]\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_Classifier'\n",
        "\n",
        "model = BERT_DCE_DANN(BERT_DCE_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQvF0tKt1h-h",
        "outputId": "bc5d9185-d1d4-4f77-848c-447831696e73"
      },
      "outputs": [],
      "source": [
        "# BERT DBCE - celoss expr - just bal dom label\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DCE_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # domain_1_loss = nn.CrossEntropyLoss()\n",
        "    domain_1_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom1_pos_prior, dom1_neg_prior]))\n",
        "    # domain_1_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_1_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom1_pos_prior, dom1_neg_prior]))\n",
        "    # domain_2_loss = nn.CrossEntropyLoss()\n",
        "    domain_2_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom2_pos_prior, dom2_neg_prior]))\n",
        "    # domain_2_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_2_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom2_pos_prior, dom2_neg_prior]))\n",
        "    domain_prior = [0.5, 0.5]\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_Classifier'\n",
        "\n",
        "model = BERT_DCE_DANN(BERT_DCE_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rjyu7oF1nT5",
        "outputId": "227354a5-a25c-4a94-e125-5a201ff52f90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12600/12600 [00:00<00:00, 62878.33it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 5744.28it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 135326.00it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 119904.92it/s]\n",
            "100%|██████████| 12600/12600 [00:00<00:00, 42686.88it/s]\n",
            "100%|██████████| 12600/12600 [00:00<00:00, 53734.31it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 52628.84it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 54578.59it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 53698.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class prior: 0.2222222222222222 0.7777777777777778\n",
            "domain prior: 0.7222222222222222 0.2777777777777778\n",
            "dom1 class prior: 0.5 0.5\n",
            "dom2 class prior: 0.11538461538461539 0.8846153846153846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12600/12600 [00:03<00:00, 4084.71it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 6401.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Train | Loss:  0.0962 | Accuracy:  0.5616| F1:  0.3621 | Balanced Accuracy:  0.5610 | Dom Avg Accuracy:  0.5796 |\n",
            "                    Domain 1 Accuracy:  0.5297| Domain 1 F1:  0.5024 | Domain 1 Balanced Accuracy:  0.5297 | \n",
            "                    Domain 2 Accuracy:  0.5738| Domain 2 F1:  0.2754 | Domain 2 Balanced Accuracy:  0.6295\n",
            "Epoch 1 Val   | Loss:  0.2324 | Accuracy:  0.6689| F1:  0.3568 | Balanced Accuracy:  0.5776 | Dom Avg Accuracy:  0.6131 |\n",
            "                Domain 1 Accuracy:  0.5547| Domain 1 F1:  0.3949 | Domain 1 Balanced Accuracy:  0.5547 | \n",
            "                Domain 2 Accuracy:  0.7128| Domain 2 F1:  0.3317 | Domain 2 Balanced Accuracy:  0.6715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Train | Loss:  0.0917 | Accuracy:  0.6575| F1:  0.4320 | Balanced Accuracy:  0.6320 | Dom Avg Accuracy:  0.6552 |\n",
            "                    Domain 1 Accuracy:  0.5820| Domain 1 F1:  0.5282 | Domain 1 Balanced Accuracy:  0.5820 | \n",
            "                    Domain 2 Accuracy:  0.6865| Domain 2 F1:  0.3656 | Domain 2 Balanced Accuracy:  0.7284\n",
            "Epoch 2 Val   | Loss:  0.2214 | Accuracy:  0.6959| F1:  0.4263 | Balanced Accuracy:  0.6289 | Dom Avg Accuracy:  0.6561 |\n",
            "                Domain 1 Accuracy:  0.5907| Domain 1 F1:  0.4892 | Domain 1 Balanced Accuracy:  0.5907 | \n",
            "                Domain 2 Accuracy:  0.7364| Domain 2 F1:  0.3807 | Domain 2 Balanced Accuracy:  0.7215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Train | Loss:  0.0884 | Accuracy:  0.6679| F1:  0.4532 | Balanced Accuracy:  0.6505 | Dom Avg Accuracy:  0.6795 |\n",
            "                    Domain 1 Accuracy:  0.6157| Domain 1 F1:  0.5663 | Domain 1 Balanced Accuracy:  0.6157 | \n",
            "                    Domain 2 Accuracy:  0.6879| Domain 2 F1:  0.3761 | Domain 2 Balanced Accuracy:  0.7433\n",
            "Epoch 3 Val   | Loss:  0.2194 | Accuracy:  0.6996| F1:  0.4380 | Balanced Accuracy:  0.6379 | Dom Avg Accuracy:  0.6656 |\n",
            "                Domain 1 Accuracy:  0.6013| Domain 1 F1:  0.5074 | Domain 1 Balanced Accuracy:  0.6013 | \n",
            "                Domain 2 Accuracy:  0.7374| Domain 2 F1:  0.3876 | Domain 2 Balanced Accuracy:  0.7299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Train | Loss:  0.0849 | Accuracy:  0.6648| F1:  0.4687 | Balanced Accuracy:  0.6650 | Dom Avg Accuracy:  0.6941 |\n",
            "                    Domain 1 Accuracy:  0.6414| Domain 1 F1:  0.6094 | Domain 1 Balanced Accuracy:  0.6414 | \n",
            "                    Domain 2 Accuracy:  0.6737| Domain 2 F1:  0.3732 | Domain 2 Balanced Accuracy:  0.7469\n",
            "Epoch 4 Val   | Loss:  0.2117 | Accuracy:  0.5904| F1:  0.4459 | Balanced Accuracy:  0.6444 | Dom Avg Accuracy:  0.6630 |\n",
            "                Domain 1 Accuracy:  0.6200| Domain 1 F1:  0.6360 | Domain 1 Balanced Accuracy:  0.6200 | \n",
            "                Domain 2 Accuracy:  0.5790| Domain 2 F1:  0.3232 | Domain 2 Balanced Accuracy:  0.7060\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Train | Loss:  0.0815 | Accuracy:  0.6787| F1:  0.4934 | Balanced Accuracy:  0.6877 | Dom Avg Accuracy:  0.7144 |\n",
            "                    Domain 1 Accuracy:  0.6771| Domain 1 F1:  0.6572 | Domain 1 Balanced Accuracy:  0.6771 | \n",
            "                    Domain 2 Accuracy:  0.6793| Domain 2 F1:  0.3784 | Domain 2 Balanced Accuracy:  0.7517\n",
            "Epoch 5 Val   | Loss:  0.2187 | Accuracy:  0.5293| F1:  0.4621 | Balanced Accuracy:  0.6652 | Dom Avg Accuracy:  0.6724 |\n",
            "                Domain 1 Accuracy:  0.6533| Domain 1 F1:  0.7168 | Domain 1 Balanced Accuracy:  0.6533 | \n",
            "                Domain 2 Accuracy:  0.4815| Domain 2 F1:  0.3003 | Domain 2 Balanced Accuracy:  0.6915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Train | Loss:  0.0790 | Accuracy:  0.6821| F1:  0.5123 | Balanced Accuracy:  0.7068 | Dom Avg Accuracy:  0.7266 |\n",
            "                    Domain 1 Accuracy:  0.6960| Domain 1 F1:  0.6927 | Domain 1 Balanced Accuracy:  0.6960 | \n",
            "                    Domain 2 Accuracy:  0.6767| Domain 2 F1:  0.3809 | Domain 2 Balanced Accuracy:  0.7572\n",
            "Epoch 6 Val   | Loss:  0.2062 | Accuracy:  0.7248| F1:  0.4976 | Balanced Accuracy:  0.6850 | Dom Avg Accuracy:  0.6989 |\n",
            "                Domain 1 Accuracy:  0.6613| Domain 1 F1:  0.6186 | Domain 1 Balanced Accuracy:  0.6613 | \n",
            "                Domain 2 Accuracy:  0.7492| Domain 2 F1:  0.3985 | Domain 2 Balanced Accuracy:  0.7365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Train | Loss:  0.0760 | Accuracy:  0.7031| F1:  0.5374 | Balanced Accuracy:  0.7292 | Dom Avg Accuracy:  0.7455 |\n",
            "                    Domain 1 Accuracy:  0.7194| Domain 1 F1:  0.7197 | Domain 1 Balanced Accuracy:  0.7194 | \n",
            "                    Domain 2 Accuracy:  0.6968| Domain 2 F1:  0.3980 | Domain 2 Balanced Accuracy:  0.7715\n",
            "Epoch 7 Val   | Loss:  0.1905 | Accuracy:  0.6778| F1:  0.5256 | Balanced Accuracy:  0.7226 | Dom Avg Accuracy:  0.7377 |\n",
            "                Domain 1 Accuracy:  0.7013| Domain 1 F1:  0.7121 | Domain 1 Balanced Accuracy:  0.7013 | \n",
            "                Domain 2 Accuracy:  0.6687| Domain 2 F1:  0.3883 | Domain 2 Balanced Accuracy:  0.7741\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 Train | Loss:  0.0732 | Accuracy:  0.7150| F1:  0.5511 | Balanced Accuracy:  0.7408 | Dom Avg Accuracy:  0.7538 |\n",
            "                    Domain 1 Accuracy:  0.7351| Domain 1 F1:  0.7378 | Domain 1 Balanced Accuracy:  0.7351 | \n",
            "                    Domain 2 Accuracy:  0.7073| Domain 2 F1:  0.4032 | Domain 2 Balanced Accuracy:  0.7724\n",
            "Epoch 8 Val   | Loss:  0.1841 | Accuracy:  0.6885| F1:  0.5336 | Balanced Accuracy:  0.7289 | Dom Avg Accuracy:  0.7456 |\n",
            "                Domain 1 Accuracy:  0.7107| Domain 1 F1:  0.7178 | Domain 1 Balanced Accuracy:  0.7107 | \n",
            "                Domain 2 Accuracy:  0.6800| Domain 2 F1:  0.3965 | Domain 2 Balanced Accuracy:  0.7805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 Train | Loss:  0.0706 | Accuracy:  0.7234| F1:  0.5594 | Balanced Accuracy:  0.7472 | Dom Avg Accuracy:  0.7620 |\n",
            "                    Domain 1 Accuracy:  0.7440| Domain 1 F1:  0.7444 | Domain 1 Balanced Accuracy:  0.7440 | \n",
            "                    Domain 2 Accuracy:  0.7155| Domain 2 F1:  0.4120 | Domain 2 Balanced Accuracy:  0.7800\n",
            "Epoch 9 Val   | Loss:  0.1780 | Accuracy:  0.6852| F1:  0.5498 | Balanced Accuracy:  0.7494 | Dom Avg Accuracy:  0.7499 |\n",
            "                Domain 1 Accuracy:  0.7200| Domain 1 F1:  0.7482 | Domain 1 Balanced Accuracy:  0.7200 | \n",
            "                Domain 2 Accuracy:  0.6718| Domain 2 F1:  0.3928 | Domain 2 Balanced Accuracy:  0.7797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 Train | Loss:  0.0682 | Accuracy:  0.7470| F1:  0.5832 | Balanced Accuracy:  0.7646 | Dom Avg Accuracy:  0.7731 |\n",
            "                    Domain 1 Accuracy:  0.7577| Domain 1 F1:  0.7596 | Domain 1 Balanced Accuracy:  0.7577 | \n",
            "                    Domain 2 Accuracy:  0.7429| Domain 2 F1:  0.4320 | Domain 2 Balanced Accuracy:  0.7884\n",
            "Epoch 10 Val   | Loss:  0.1841 | Accuracy:  0.6648| F1:  0.5366 | Balanced Accuracy:  0.7393 | Dom Avg Accuracy:  0.7493 |\n",
            "                Domain 1 Accuracy:  0.7267| Domain 1 F1:  0.7527 | Domain 1 Balanced Accuracy:  0.7267 | \n",
            "                Domain 2 Accuracy:  0.6410| Domain 2 F1:  0.3772 | Domain 2 Balanced Accuracy:  0.7720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 Train | Loss:  0.0653 | Accuracy:  0.7506| F1:  0.5901 | Balanced Accuracy:  0.7711 | Dom Avg Accuracy:  0.7822 |\n",
            "                    Domain 1 Accuracy:  0.7680| Domain 1 F1:  0.7693 | Domain 1 Balanced Accuracy:  0.7680 | \n",
            "                    Domain 2 Accuracy:  0.7440| Domain 2 F1:  0.4380 | Domain 2 Balanced Accuracy:  0.7965\n",
            "Epoch 11 Val   | Loss:  0.1693 | Accuracy:  0.7344| F1:  0.5795 | Balanced Accuracy:  0.7662 | Dom Avg Accuracy:  0.7690 |\n",
            "                Domain 1 Accuracy:  0.7493| Domain 1 F1:  0.7608 | Domain 1 Balanced Accuracy:  0.7493 | \n",
            "                Domain 2 Accuracy:  0.7287| Domain 2 F1:  0.4244 | Domain 2 Balanced Accuracy:  0.7887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 Train | Loss:  0.0630 | Accuracy:  0.7627| F1:  0.6044 | Balanced Accuracy:  0.7816 | Dom Avg Accuracy:  0.7918 |\n",
            "                    Domain 1 Accuracy:  0.7794| Domain 1 F1:  0.7807 | Domain 1 Balanced Accuracy:  0.7794 | \n",
            "                    Domain 2 Accuracy:  0.7563| Domain 2 F1:  0.4507 | Domain 2 Balanced Accuracy:  0.8043\n",
            "Epoch 12 Val   | Loss:  0.1726 | Accuracy:  0.7289| F1:  0.5679 | Balanced Accuracy:  0.7549 | Dom Avg Accuracy:  0.7672 |\n",
            "                Domain 1 Accuracy:  0.7453| Domain 1 F1:  0.7484 | Domain 1 Balanced Accuracy:  0.7453 | \n",
            "                Domain 2 Accuracy:  0.7226| Domain 2 F1:  0.4214 | Domain 2 Balanced Accuracy:  0.7891\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 Train | Loss:  0.0615 | Accuracy:  0.7693| F1:  0.6152 | Balanced Accuracy:  0.7910 | Dom Avg Accuracy:  0.8001 |\n",
            "                    Domain 1 Accuracy:  0.7966| Domain 1 F1:  0.7994 | Domain 1 Balanced Accuracy:  0.7966 | \n",
            "                    Domain 2 Accuracy:  0.7588| Domain 2 F1:  0.4519 | Domain 2 Balanced Accuracy:  0.8036\n",
            "Epoch 13 Val   | Loss:  0.1720 | Accuracy:  0.7222| F1:  0.5758 | Balanced Accuracy:  0.7673 | Dom Avg Accuracy:  0.7755 |\n",
            "                Domain 1 Accuracy:  0.7560| Domain 1 F1:  0.7692 | Domain 1 Balanced Accuracy:  0.7560 | \n",
            "                Domain 2 Accuracy:  0.7092| Domain 2 F1:  0.4185 | Domain 2 Balanced Accuracy:  0.7951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 Train | Loss:  0.0595 | Accuracy:  0.7738| F1:  0.6208 | Balanced Accuracy:  0.7950 | Dom Avg Accuracy:  0.8066 |\n",
            "                    Domain 1 Accuracy:  0.8003| Domain 1 F1:  0.8016 | Domain 1 Balanced Accuracy:  0.8003 | \n",
            "                    Domain 2 Accuracy:  0.7636| Domain 2 F1:  0.4613 | Domain 2 Balanced Accuracy:  0.8130\n",
            "Epoch 14 Val   | Loss:  0.1801 | Accuracy:  0.6863| F1:  0.5618 | Balanced Accuracy:  0.7644 | Dom Avg Accuracy:  0.7621 |\n",
            "                Domain 1 Accuracy:  0.7453| Domain 1 F1:  0.7776 | Domain 1 Balanced Accuracy:  0.7453 | \n",
            "                Domain 2 Accuracy:  0.6636| Domain 2 F1:  0.3892 | Domain 2 Balanced Accuracy:  0.7789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 Train | Loss:  0.0579 | Accuracy:  0.7797| F1:  0.6302 | Balanced Accuracy:  0.8029 | Dom Avg Accuracy:  0.8116 |\n",
            "                    Domain 1 Accuracy:  0.8134| Domain 1 F1:  0.8168 | Domain 1 Balanced Accuracy:  0.8134 | \n",
            "                    Domain 2 Accuracy:  0.7667| Domain 2 F1:  0.4613 | Domain 2 Balanced Accuracy:  0.8098\n",
            "Epoch 15 Val   | Loss:  0.1759 | Accuracy:  0.6996| F1:  0.5656 | Balanced Accuracy:  0.7640 | Dom Avg Accuracy:  0.7779 |\n",
            "                Domain 1 Accuracy:  0.7600| Domain 1 F1:  0.7772 | Domain 1 Balanced Accuracy:  0.7600 | \n",
            "                Domain 2 Accuracy:  0.6764| Domain 2 F1:  0.4042 | Domain 2 Balanced Accuracy:  0.7958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 Train | Loss:  0.0557 | Accuracy:  0.7902| F1:  0.6435 | Balanced Accuracy:  0.8122 | Dom Avg Accuracy:  0.8225 |\n",
            "                    Domain 1 Accuracy:  0.8254| Domain 1 F1:  0.8275 | Domain 1 Balanced Accuracy:  0.8254 | \n",
            "                    Domain 2 Accuracy:  0.7767| Domain 2 F1:  0.4749 | Domain 2 Balanced Accuracy:  0.8195\n",
            "Epoch 16 Val   | Loss:  0.1698 | Accuracy:  0.6911| F1:  0.5688 | Balanced Accuracy:  0.7717 | Dom Avg Accuracy:  0.7619 |\n",
            "                Domain 1 Accuracy:  0.7373| Domain 1 F1:  0.7754 | Domain 1 Balanced Accuracy:  0.7373 | \n",
            "                Domain 2 Accuracy:  0.6733| Domain 2 F1:  0.3974 | Domain 2 Balanced Accuracy:  0.7864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 Train | Loss:  0.0539 | Accuracy:  0.7983| F1:  0.6560 | Balanced Accuracy:  0.8223 | Dom Avg Accuracy:  0.8315 |\n",
            "                    Domain 1 Accuracy:  0.8334| Domain 1 F1:  0.8365 | Domain 1 Balanced Accuracy:  0.8334 | \n",
            "                    Domain 2 Accuracy:  0.7848| Domain 2 F1:  0.4877 | Domain 2 Balanced Accuracy:  0.8295\n",
            "Epoch 17 Val   | Loss:  0.1690 | Accuracy:  0.7107| F1:  0.5799 | Balanced Accuracy:  0.7777 | Dom Avg Accuracy:  0.7797 |\n",
            "                Domain 1 Accuracy:  0.7560| Domain 1 F1:  0.7808 | Domain 1 Balanced Accuracy:  0.7560 | \n",
            "                Domain 2 Accuracy:  0.6933| Domain 2 F1:  0.4160 | Domain 2 Balanced Accuracy:  0.8035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 Train | Loss:  0.0519 | Accuracy:  0.8015| F1:  0.6600 | Balanced Accuracy:  0.8248 | Dom Avg Accuracy:  0.8355 |\n",
            "                    Domain 1 Accuracy:  0.8411| Domain 1 F1:  0.8433 | Domain 1 Balanced Accuracy:  0.8411 | \n",
            "                    Domain 2 Accuracy:  0.7863| Domain 2 F1:  0.4891 | Domain 2 Balanced Accuracy:  0.8299\n",
            "Epoch 18 Val   | Loss:  0.1729 | Accuracy:  0.7437| F1:  0.5831 | Balanced Accuracy:  0.7662 | Dom Avg Accuracy:  0.7745 |\n",
            "                Domain 1 Accuracy:  0.7467| Domain 1 F1:  0.7507 | Domain 1 Balanced Accuracy:  0.7467 | \n",
            "                Domain 2 Accuracy:  0.7426| Domain 2 F1:  0.4410 | Domain 2 Balanced Accuracy:  0.8023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 Train | Loss:  0.0498 | Accuracy:  0.8131| F1:  0.6765 | Balanced Accuracy:  0.8367 | Dom Avg Accuracy:  0.8461 |\n",
            "                    Domain 1 Accuracy:  0.8529| Domain 1 F1:  0.8555 | Domain 1 Balanced Accuracy:  0.8529 | \n",
            "                    Domain 2 Accuracy:  0.7978| Domain 2 F1:  0.5048 | Domain 2 Balanced Accuracy:  0.8393\n",
            "Epoch 19 Val   | Loss:  0.1629 | Accuracy:  0.7585| F1:  0.6000 | Balanced Accuracy:  0.7787 | Dom Avg Accuracy:  0.7771 |\n",
            "                Domain 1 Accuracy:  0.7533| Domain 1 F1:  0.7625 | Domain 1 Balanced Accuracy:  0.7533 | \n",
            "                Domain 2 Accuracy:  0.7605| Domain 2 F1:  0.4512 | Domain 2 Balanced Accuracy:  0.8009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 Train | Loss:  0.0484 | Accuracy:  0.8149| F1:  0.6791 | Balanced Accuracy:  0.8387 | Dom Avg Accuracy:  0.8491 |\n",
            "                    Domain 1 Accuracy:  0.8546| Domain 1 F1:  0.8567 | Domain 1 Balanced Accuracy:  0.8546 | \n",
            "                    Domain 2 Accuracy:  0.7997| Domain 2 F1:  0.5093 | Domain 2 Balanced Accuracy:  0.8437\n",
            "Epoch 20 Val   | Loss:  0.1793 | Accuracy:  0.6919| F1:  0.5689 | Balanced Accuracy:  0.7715 | Dom Avg Accuracy:  0.7703 |\n",
            "                Domain 1 Accuracy:  0.7533| Domain 1 F1:  0.7846 | Domain 1 Balanced Accuracy:  0.7533 | \n",
            "                Domain 2 Accuracy:  0.6682| Domain 2 F1:  0.3959 | Domain 2 Balanced Accuracy:  0.7873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 Train | Loss:  0.0463 | Accuracy:  0.8238| F1:  0.6910 | Balanced Accuracy:  0.8462 | Dom Avg Accuracy:  0.8555 |\n",
            "                    Domain 1 Accuracy:  0.8683| Domain 1 F1:  0.8704 | Domain 1 Balanced Accuracy:  0.8683 | \n",
            "                    Domain 2 Accuracy:  0.8067| Domain 2 F1:  0.5150 | Domain 2 Balanced Accuracy:  0.8427\n",
            "Epoch 21 Val   | Loss:  0.1687 | Accuracy:  0.7459| F1:  0.5922 | Balanced Accuracy:  0.7760 | Dom Avg Accuracy:  0.7878 |\n",
            "                Domain 1 Accuracy:  0.7760| Domain 1 F1:  0.7807 | Domain 1 Balanced Accuracy:  0.7760 | \n",
            "                Domain 2 Accuracy:  0.7344| Domain 2 F1:  0.4345 | Domain 2 Balanced Accuracy:  0.7996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 Train | Loss:  0.0438 | Accuracy:  0.8272| F1:  0.6975 | Balanced Accuracy:  0.8519 | Dom Avg Accuracy:  0.8624 |\n",
            "                    Domain 1 Accuracy:  0.8783| Domain 1 F1:  0.8804 | Domain 1 Balanced Accuracy:  0.8783 | \n",
            "                    Domain 2 Accuracy:  0.8076| Domain 2 F1:  0.5183 | Domain 2 Balanced Accuracy:  0.8465\n",
            "Epoch 22 Val   | Loss:  0.1759 | Accuracy:  0.7385| F1:  0.5905 | Balanced Accuracy:  0.7777 | Dom Avg Accuracy:  0.7873 |\n",
            "                Domain 1 Accuracy:  0.7600| Domain 1 F1:  0.7698 | Domain 1 Balanced Accuracy:  0.7600 | \n",
            "                Domain 2 Accuracy:  0.7303| Domain 2 F1:  0.4416 | Domain 2 Balanced Accuracy:  0.8147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 Train | Loss:  0.0428 | Accuracy:  0.8307| F1:  0.7026 | Balanced Accuracy:  0.8555 | Dom Avg Accuracy:  0.8664 |\n",
            "                    Domain 1 Accuracy:  0.8817| Domain 1 F1:  0.8836 | Domain 1 Balanced Accuracy:  0.8817 | \n",
            "                    Domain 2 Accuracy:  0.8111| Domain 2 F1:  0.5245 | Domain 2 Balanced Accuracy:  0.8510\n",
            "Epoch 23 Val   | Loss:  0.1843 | Accuracy:  0.7774| F1:  0.6160 | Balanced Accuracy:  0.7867 | Dom Avg Accuracy:  0.7893 |\n",
            "                Domain 1 Accuracy:  0.7707| Domain 1 F1:  0.7725 | Domain 1 Balanced Accuracy:  0.7707 | \n",
            "                Domain 2 Accuracy:  0.7800| Domain 2 F1:  0.4697 | Domain 2 Balanced Accuracy:  0.8080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 Train | Loss:  0.0406 | Accuracy:  0.8390| F1:  0.7132 | Balanced Accuracy:  0.8611 | Dom Avg Accuracy:  0.8719 |\n",
            "                    Domain 1 Accuracy:  0.8877| Domain 1 F1:  0.8891 | Domain 1 Balanced Accuracy:  0.8877 | \n",
            "                    Domain 2 Accuracy:  0.8202| Domain 2 F1:  0.5368 | Domain 2 Balanced Accuracy:  0.8561\n",
            "Epoch 24 Val   | Loss:  0.1700 | Accuracy:  0.7911| F1:  0.6250 | Balanced Accuracy:  0.7883 | Dom Avg Accuracy:  0.7846 |\n",
            "                Domain 1 Accuracy:  0.7720| Domain 1 F1:  0.7729 | Domain 1 Balanced Accuracy:  0.7720 | \n",
            "                Domain 2 Accuracy:  0.7985| Domain 2 F1:  0.4767 | Domain 2 Balanced Accuracy:  0.7972\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 Train | Loss:  0.0379 | Accuracy:  0.8463| F1:  0.7254 | Balanced Accuracy:  0.8704 | Dom Avg Accuracy:  0.8811 |\n",
            "                    Domain 1 Accuracy:  0.9034| Domain 1 F1:  0.9050 | Domain 1 Balanced Accuracy:  0.9034 | \n",
            "                    Domain 2 Accuracy:  0.8243| Domain 2 F1:  0.5428 | Domain 2 Balanced Accuracy:  0.8589\n",
            "Epoch 25 Val   | Loss:  0.1830 | Accuracy:  0.7489| F1:  0.5891 | Balanced Accuracy:  0.7707 | Dom Avg Accuracy:  0.7707 |\n",
            "                Domain 1 Accuracy:  0.7467| Domain 1 F1:  0.7558 | Domain 1 Balanced Accuracy:  0.7467 | \n",
            "                Domain 2 Accuracy:  0.7497| Domain 2 F1:  0.4404 | Domain 2 Balanced Accuracy:  0.7948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 1575/1576 [01:04<00:00, 24.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 Train | Loss:  0.0373 | Accuracy:  0.8544| F1:  0.7369 | Balanced Accuracy:  0.8770 | Dom Avg Accuracy:  0.8878 |\n",
            "                    Domain 1 Accuracy:  0.9086| Domain 1 F1:  0.9098 | Domain 1 Balanced Accuracy:  0.9086 | \n",
            "                    Domain 2 Accuracy:  0.8335| Domain 2 F1:  0.5579 | Domain 2 Balanced Accuracy:  0.8670\n",
            "Epoch 26 Val   | Loss:  0.1916 | Accuracy:  0.7333| F1:  0.5804 | Balanced Accuracy:  0.7679 | Dom Avg Accuracy:  0.7759 |\n",
            "                Domain 1 Accuracy:  0.7507| Domain 1 F1:  0.7599 | Domain 1 Balanced Accuracy:  0.7507 | \n",
            "                Domain 2 Accuracy:  0.7267| Domain 2 F1:  0.4312 | Domain 2 Balanced Accuracy:  0.8011\n",
            "\n",
            "Epoch 24 Val   | Loss:  0.1700 | Accuracy:  0.7911| F1:  0.6250 | Balanced Accuracy:  0.7883 | Dom Avg Accuracy:  0.7846 |\n",
            "                Domain 1 Accuracy:  0.7720| Domain 1 F1:  0.7729 | Domain 1 Balanced Accuracy:  0.7720 | \n",
            "                Domain 2 Accuracy:  0.7985| Domain 2 F1:  0.4767 | Domain 2 Balanced Accuracy:  0.7972\n",
            "Epoch 24 Val   | Loss:  0.1684 | Accuracy:  0.7952| F1:  0.6271 | Balanced Accuracy:  0.7880 | Dom Avg Accuracy:  0.7799 |\n",
            "                Domain 1 Accuracy:  0.7533| Domain 1 F1:  0.7550 | Domain 1 Balanced Accuracy:  0.7533 | \n",
            "                Domain 2 Accuracy:  0.8113| Domain 2 F1:  0.4945 | Domain 2 Balanced Accuracy:  0.8064\n"
          ]
        }
      ],
      "source": [
        "# BERT DBCE - celoss expr - bal both domain and label\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DCE_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 8\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "    loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # validation_loss = nn.CrossEntropyLoss()\n",
        "    validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # domain_1_loss = nn.CrossEntropyLoss()\n",
        "    domain_1_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom1_pos_prior, dom1_neg_prior]))\n",
        "    # domain_1_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_1_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom1_pos_prior, dom1_neg_prior]))\n",
        "    # domain_2_loss = nn.CrossEntropyLoss()\n",
        "    domain_2_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom2_pos_prior, dom2_neg_prior]))\n",
        "    # domain_2_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_2_validation_loss = nn.CrossEntropyLoss(weight=torch.FloatTensor([dom2_pos_prior, dom2_neg_prior]))\n",
        "    domain_prior = [pos_dom_prior, neg_dom_prior]\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    domain_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_Classifier'\n",
        "\n",
        "model = BERT_DCE_DANN(BERT_DCE_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ6O6vc_RDlN"
      },
      "source": [
        "---\n",
        "# Hinge Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RKgxTV4fQ_ck"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12600/12600 [00:00<00:00, 22925.52it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 31301.90it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 8854.81it/s] \n",
            "100%|██████████| 4000/4000 [00:00<00:00, 19476.57it/s]\n",
            "100%|██████████| 12600/12600 [00:01<00:00, 7186.27it/s]\n",
            "100%|██████████| 12600/12600 [00:00<00:00, 43106.40it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 47805.40it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 33598.43it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 29364.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class prior: 0.75 0.25\n",
            "domain prior: 0.0 1.0\n",
            "dom1 class prior: 0.5 0.5\n",
            "dom2 class prior: 0.11538461538461539 0.8846153846153846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12600/12600 [00:02<00:00, 5948.11it/s]\n",
            "100%|██████████| 2700/2700 [00:00<00:00, 4334.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            " 50%|█████     | 1/2 [00:01<00:01,  1.79s/it]\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Train | Loss:  0.9633 | Accuracy:  0.7500| F1:  0.8571 | Balanced Accuracy:  0.5000 | Dom Avg Accuracy:     nan |\n",
            "                    Domain 1 Accuracy:  0.7500| Domain 1 F1:  0.8571 | Domain 1 Balanced Accuracy:  0.5000 | \n",
            "                    Domain 2 Accuracy:     nan| Domain 2 F1:  0.0000 | Domain 2 Balanced Accuracy:     nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Val   | Loss:  1.1302 | Accuracy:  0.3750| F1:  0.5455 | Balanced Accuracy:  0.5000 | Dom Avg Accuracy:     nan |\n",
            "                Domain 1 Accuracy:  0.3750| Domain 1 F1:  0.5455 | Domain 1 Balanced Accuracy:  0.5000 | \n",
            "                Domain 2 Accuracy:     nan| Domain 2 F1:  0.0000 | Domain 2 Balanced Accuracy:     nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  1.43s/it]\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Train | Loss:  0.7216 | Accuracy:  0.7500| F1:  0.8571 | Balanced Accuracy:  0.5000 | Dom Avg Accuracy:     nan |\n",
            "                    Domain 1 Accuracy:  0.7500| Domain 1 F1:  0.8571 | Domain 1 Balanced Accuracy:  0.5000 | \n",
            "                    Domain 2 Accuracy:     nan| Domain 2 F1:  0.0000 | Domain 2 Balanced Accuracy:     nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Val   | Loss:  1.4460 | Accuracy:  0.3750| F1:  0.5455 | Balanced Accuracy:  0.5000 | Dom Avg Accuracy:     nan |\n",
            "                Domain 1 Accuracy:  0.3750| Domain 1 F1:  0.5455 | Domain 1 Balanced Accuracy:  0.5000 | \n",
            "                Domain 2 Accuracy:     nan| Domain 2 F1:  0.0000 | Domain 2 Balanced Accuracy:     nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:02<00:02,  2.20s/it]\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Train | Loss:  0.5789 | Accuracy:  0.7500| F1:  0.8571 | Balanced Accuracy:  0.5000 | Dom Avg Accuracy:     nan |\n",
            "                    Domain 1 Accuracy:  0.7500| Domain 1 F1:  0.8571 | Domain 1 Balanced Accuracy:  0.5000 | \n",
            "                    Domain 2 Accuracy:     nan| Domain 2 F1:  0.0000 | Domain 2 Balanced Accuracy:     nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Val   | Loss:  1.2411 | Accuracy:  0.3750| F1:  0.5455 | Balanced Accuracy:  0.5000 | Dom Avg Accuracy:     nan |\n",
            "                Domain 1 Accuracy:  0.3750| Domain 1 F1:  0.5455 | Domain 1 Balanced Accuracy:  0.5000 | \n",
            "                Domain 2 Accuracy:     nan| Domain 2 F1:  0.0000 | Domain 2 Balanced Accuracy:     nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m model \u001b[38;5;241m=\u001b[39m BERT_Hinge(BERT_Hinge_config) \u001b[38;5;66;03m# initialise the model\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# train the model (all cells except this one will print training log and evaluation at each batch)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/1. University/2. Masters/7. Statistical Machine Learning/Assignments/A1/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch/model/model_class/__template__.py:1225\u001b[0m, in \u001b[0;36mHingeModel.fit\u001b[0;34m(self, total_train_X, total_train_y, total_train_domain, total_val_X, total_val_y, total_val_domain)\u001b[0m\n\u001b[1;32m   1222\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(pred, true) \n\u001b[1;32m   1224\u001b[0m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[0;32m-> 1225\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mgrad_clip: \u001b[38;5;66;03m# gradient clip\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[0;32m/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# BERT - hinge work in progress\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "\n",
        "# train_x = train_x[:8]\n",
        "# train_y = train_y[:8]\n",
        "# train_dom = train_dom[:8]\n",
        "# val_dom = val_dom[:8]\n",
        "# val_x = val_x[:8]\n",
        "# val_y = val_y[:8]\n",
        "\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_Hinge_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 6\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_layers = 1\n",
        "    # n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-3\n",
        "    patience = 10\n",
        "    # loss = nn.BCELoss()\n",
        "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    loss = HingeLoss()\n",
        "    # validation_loss = nn.BCELoss()\n",
        "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    validation_loss = HingeLoss()\n",
        "    domain_loss = nn.BCELoss()\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 1\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_Hinge'\n",
        "\n",
        "model = BERT_Hinge(BERT_Hinge_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Historic Best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT - SoftmaxBCELoss+OptBalAccu\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 6 # actually 8 was better\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.BCELoss()\n",
        "    loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # validation_loss = nn.BCELoss()\n",
        "    validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    domain_loss = nn.BCELoss()\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_DANN'\n",
        "\n",
        "model = BERT_DANN(BERT_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT - with low freq token - SoftmaxBCELoss+OptBalAccu\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 256\n",
        "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = True\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class BERT_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    d_ff = 1024 # = 4* d_model\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    e_layers = 6\n",
        "    embedding_aggregation = 'cls' # TODO\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    res_learning = False\n",
        "    activation = nn.ReLU()\n",
        "    mask_flag = False # causal mask\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 10\n",
        "    # loss = nn.BCELoss()\n",
        "    loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    # validation_loss = nn.BCELoss()\n",
        "    validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    domain_loss = nn.BCELoss()\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    gradient_reversal_every_n_epoch = 1\n",
        "    regularisation_loss = None\n",
        "    scheduler = False\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory + f'./results/'\n",
        "    name = f'BERT_DANN'\n",
        "\n",
        "model = BERT_DANN(BERT_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "# LSTM Graveyard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM\n",
        "MAX_SENTENCE_LENGTH = 64\n",
        "MIN_FREQUENCY = 0 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = False\n",
        "PAD_FRONT = True\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class LSTM_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 256\n",
        "    n_recurrent_layers = 2\n",
        "    bidirectional = False\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    n_mlp_layers = 1\n",
        "    flatten = False\n",
        "    activation = nn.ReLU()\n",
        "    res_learning = True\n",
        "    mask_flag = False # TODO\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 8\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 4\n",
        "    loss = nn.BCELoss()\n",
        "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    validation_loss = nn.BCELoss()\n",
        "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    regularisation_loss = None\n",
        "    scheduler = True\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory +  f'./results/'\n",
        "    name = f'LSTM_Classifier'\n",
        "\n",
        "\n",
        "\n",
        "model = LSTM(LSTM_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM_DANN\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 128\n",
        "MIN_FREQUENCY = 0 # because 40 is statistical sample requirement\n",
        "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
        "LOW_FREQ_TOKEN = False\n",
        "CLS = True\n",
        "PAD_FRONT = False\n",
        "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
        "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
        "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
        "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
        "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
        "                                                              cropped_val_data, \\\n",
        "                                                                cropped_test_data, \\\n",
        "                                                                    cropped_future_data, \\\n",
        "                                                                        MAX_SENTENCE_LENGTH, \\\n",
        "                                                                            raw_token_pytorch_map, \\\n",
        "                                                                                CLS=CLS, \\\n",
        "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
        "                                                                                        pad_front=PAD_FRONT)\n",
        "pos_prior, neg_prior = get_distribution(train_y)\n",
        "print('class prior:', pos_prior, neg_prior)\n",
        "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
        "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
        "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
        "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
        "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
        "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
        "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
        "\n",
        "print('---')\n",
        "\n",
        "class LSTM_DANN_config:\n",
        "    # ----------------- architectual hyperparameters ----------------- #\n",
        "    d_model = 512\n",
        "    n_recurrent_layers = 2\n",
        "    bidirectional = False\n",
        "    n_heads = 8\n",
        "    dropout = 0.1\n",
        "    n_mlp_clf_layers = 1\n",
        "    n_mlp_dom_layers = 1\n",
        "    flatten = False\n",
        "    activation = nn.ReLU()\n",
        "    res_learning = True\n",
        "    mask_flag = False # TODO\n",
        "    train_embedding = True\n",
        "    # ----------------- optimisation hyperparameters ----------------- #\n",
        "    random_state = SEED\n",
        "    batch_size = 128\n",
        "    epochs = 32\n",
        "    lr = 1e-5\n",
        "    patience = 5\n",
        "    loss = nn.BCELoss()\n",
        "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    validation_loss = nn.BCELoss()\n",
        "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
        "    domain_loss = nn.BCELoss()\n",
        "    pretrain_loss = nn.CrossEntropyLoss()\n",
        "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.1\n",
        "    regularisation_loss = None\n",
        "    scheduler = True\n",
        "    grad_clip = False\n",
        "    # ----------------- operation hyperparameters ----------------- #\n",
        "    d_output = 2\n",
        "    seq_len = MAX_SENTENCE_LENGTH\n",
        "    n_unique_tokens = len(raw_token_pytorch_map)\n",
        "    # ----------------- saving hyperparameters ----------------- #\n",
        "    rootpath = home_directory + './'\n",
        "    saving_address = home_directory +  f'./results/'\n",
        "    name = f'LSTM_DANN'\n",
        "\n",
        "model = LSTM_DANN(LSTM_DANN_config) # initialise the model\n",
        "\n",
        "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
        "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
        "print()\n",
        "\n",
        "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
        "model.load()\n",
        "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
        "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
