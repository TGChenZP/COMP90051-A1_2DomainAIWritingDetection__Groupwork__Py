{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Training UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect whether this is a google environment\n",
    "\n",
    "COLAB_ENVIRONMENT = False\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    COLAB_ENVIRONMENT = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "if COLAB_ENVIRONMENT:\n",
    "    py_file_location = \"./drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch\" # my private packages are stored here\n",
    "    home_directory = './drive/MyDrive/LAB/COMP90051-A1__Groupwork__Py/' # my home directory is stored in ./LAB of google drive\n",
    "    !pip install einops\n",
    "else:\n",
    "    py_file_location = './PrivatePackages/pytorch'\n",
    "    home_directory = './'\n",
    "\n",
    "sys.path.append(os.path.abspath(py_file_location))\n",
    "\n",
    "from environment import *\n",
    "from utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_class import LSTM, BERT, LSTM_DANN, BERT_DANN, LSTM_DBCE, BERT_DBCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "with open(home_directory + '/data/raw/comp90051-2024s1-project-1/domain1_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data1.append(json.loads(line))\n",
    "\n",
    "data2 = []\n",
    "with open(home_directory + './data/raw/comp90051-2024s1-project-1/domain2_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data2.append(json.loads(line))\n",
    "\n",
    "data_test = []\n",
    "with open(home_directory + '/data/raw/comp90051-2024s1-project-1/test_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data_test.append(json.loads(line))\n",
    "\n",
    "# create domain labels for data\n",
    "for i in range(len(data1)):\n",
    "    data1[i]['domain'] = 0\n",
    "for i in range(len(data2)):\n",
    "    data2[i]['domain'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val Test Split\n",
    "\n",
    "# get labels for stratification\n",
    "label1 = [instance['label'] for instance in data1]\n",
    "label2 = [instance['label'] for instance in data2]\n",
    "\n",
    "train_ix_1, val_test_ix_1 = train_test_split(range(len(data1)), test_size=0.3, random_state=SEED, stratify = label1)\n",
    "train_ix_2, val_test_ix_2 = train_test_split(range(len(data2)), test_size=0.3, random_state=SEED, stratify = label2)\n",
    "val_ix_1, test_ix_1 = train_test_split(val_test_ix_1, test_size=0.5, random_state=SEED, stratify = [data1[i]['label'] for i in val_test_ix_1])\n",
    "val_ix_2, test_ix_2 = train_test_split(val_test_ix_2, test_size=0.5, random_state=SEED, stratify = [data2[i]['label'] for i in val_test_ix_2])\n",
    "\n",
    "# split data according to the index from train_test_split\n",
    "train_data_1 = [data1[i] for i in train_ix_1]\n",
    "val_data_1 = [data1[i] for i in val_ix_1]\n",
    "test_data_1 = [data1[i] for i in test_ix_1]\n",
    "train_data_2 = [data2[i] for i in train_ix_2]\n",
    "val_data_2 = [data2[i] for i in val_ix_2]\n",
    "test_data_2 = [data2[i] for i in test_ix_2]\n",
    "\n",
    "# combine the data\n",
    "train_data = train_data_1 + train_data_2\n",
    "val_data = val_data_1 + val_data_2\n",
    "test_data = test_data_1 + test_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Models\n",
    "\n",
    "#### 1. Prediction only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m PAD_FRONT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m W2V_CONTEXT_WINDOW \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;66;03m# 2 to left, 2 to right\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m cropped_train_data \u001b[38;5;241m=\u001b[39m crop_sentence_length(\u001b[43mtrain_data\u001b[49m, max_sentence_length \u001b[38;5;241m=\u001b[39m MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance \u001b[38;5;241m=\u001b[39m MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n\u001b[1;32m     10\u001b[0m cropped_val_data \u001b[38;5;241m=\u001b[39m crop_sentence_length(val_data, max_sentence_length \u001b[38;5;241m=\u001b[39m  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m cropped_test_data \u001b[38;5;241m=\u001b[39m crop_sentence_length(test_data, max_sentence_length \u001b[38;5;241m=\u001b[39m MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "MAX_SENTENCE_LENGTH = 128\n",
    "MIN_FREQUENCY = 0 # because 40 is statistical sample requirement\n",
    "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
    "LOW_FREQ_TOKEN = False\n",
    "CLS = True\n",
    "PAD_FRONT = False\n",
    "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
    "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
    "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY) \n",
    "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
    "                                                              cropped_val_data, \\\n",
    "                                                                cropped_test_data, \\\n",
    "                                                                    cropped_future_data, \\\n",
    "                                                                        MAX_SENTENCE_LENGTH, \\\n",
    "                                                                            raw_token_pytorch_map, \\\n",
    "                                                                                CLS=CLS, \\\n",
    "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
    "                                                                                        pad_front=PAD_FRONT)\n",
    "pos_prior, neg_prior = get_distribution(train_y)\n",
    "print('class prior:', pos_prior, neg_prior)\n",
    "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
    "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
    "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
    "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
    "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
    "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
    "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
    "\n",
    "print('---')\n",
    "\n",
    "class LSTM_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 256\n",
    "    n_recurrent_layers = 2\n",
    "    bidirectional = False\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    n_mlp_layers = 1\n",
    "    flatten = False\n",
    "    activation = nn.ReLU()\n",
    "    res_learning = True\n",
    "    mask_flag = False # TODO\n",
    "    train_embedding = True\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 8\n",
    "    epochs = 32\n",
    "    lr = 1e-5\n",
    "    patience = 3\n",
    "    loss = nn.BCELoss()\n",
    "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    validation_loss = nn.BCELoss()\n",
    "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    pretrain_loss = nn.CrossEntropyLoss()\n",
    "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
    "    regularisation_loss = None\n",
    "    scheduler = True\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = MAX_SENTENCE_LENGTH\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory + './'\n",
    "    saving_address = home_directory +  f'./results/'\n",
    "    name = f'LSTM_Classifier'\n",
    "    \n",
    "\n",
    "\n",
    "model = LSTM(LSTM_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
    "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 512\n",
    "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
    "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
    "LOW_FREQ_TOKEN = False\n",
    "CLS = True\n",
    "PAD_FRONT = False\n",
    "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
    "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
    "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY) \n",
    "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
    "                                                              cropped_val_data, \\\n",
    "                                                                cropped_test_data, \\\n",
    "                                                                    cropped_future_data, \\\n",
    "                                                                        MAX_SENTENCE_LENGTH, \\\n",
    "                                                                            raw_token_pytorch_map, \\\n",
    "                                                                                CLS=CLS, \\\n",
    "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
    "                                                                                        pad_front=PAD_FRONT)\n",
    "pos_prior, neg_prior = get_distribution(train_y)\n",
    "print('class prior:', pos_prior, neg_prior)\n",
    "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
    "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
    "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
    "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
    "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
    "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
    "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
    "\n",
    "print('---')\n",
    "\n",
    "class BERT_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 256\n",
    "    d_ff = 512 # = 4* d_model\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    e_layers = 3 \n",
    "    embedding_aggregation = 'cls' # TODO\n",
    "    n_mlp_layers = 1\n",
    "    res_learning = True\n",
    "    activation = nn.ReLU()\n",
    "    mask_flag = False # causal mask\n",
    "    train_embedding = True\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 8\n",
    "    epochs = 32\n",
    "    lr = 1e-5\n",
    "    patience = 3\n",
    "    loss = nn.BCELoss()\n",
    "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    validation_loss = nn.BCELoss()\n",
    "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    pretrain_loss = nn.CrossEntropyLoss()\n",
    "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
    "    regularisation_loss = None\n",
    "    scheduler = False\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = MAX_SENTENCE_LENGTH\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory + './'\n",
    "    saving_address = home_directory + f'./results/'\n",
    "    name = f'BERT_Classifier'\n",
    "\n",
    "model = BERT(BERT_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
    "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [00:00<00:00, 32223.16it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 105042.40it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 139832.58it/s]\n",
      "100%|██████████| 4000/4000 [00:00<00:00, 97670.29it/s]\n",
      "100%|██████████| 12600/12600 [00:00<00:00, 46628.67it/s]\n",
      "100%|██████████| 12600/12600 [00:00<00:00, 49503.62it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 49286.34it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 50240.99it/s]\n",
      "100%|██████████| 4000/4000 [00:00<00:00, 50987.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class prior: 0.2222222222222222 0.7777777777777778\n",
      "domain prior: 0.7222222222222222 0.2777777777777778\n",
      "dom1 class prior: 0.5 0.5\n",
      "dom2 class prior: 0.11538461538461539 0.8846153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [00:02<00:00, 6129.61it/s]\n",
      "100%|██████████| 2700/2700 [00:00<00:00, 4748.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 13/13 [00:02<00:00,  5.62it/s]\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train | Loss:  8.2822 | Accuracy:  0.0000| Domain 1 Accuracy:  0.0000 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val   | Loss:  8.2839 | Accuracy:  0.0000 | \n",
      "                            Domain 1 Accuracy:  0.0000 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  5.57it/s]\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train | Loss:  8.2698 | Accuracy:  0.0000| Domain 1 Accuracy:  0.0000 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Val   | Loss:  8.2739 | Accuracy:  0.0000 | \n",
      "                            Domain 1 Accuracy:  0.0000 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  5.64it/s]\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train | Loss:  8.2504 | Accuracy:  0.0000| Domain 1 Accuracy:  0.0000 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Val   | Loss:  8.2640 | Accuracy:  0.0000 | \n",
      "                            Domain 1 Accuracy:  0.0000 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  5.71it/s]\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train | Loss:  8.2356 | Accuracy:  0.0000| Domain 1 Accuracy:  0.0000 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Val   | Loss:  8.2547 | Accuracy:  0.0000 | \n",
      "                            Domain 1 Accuracy:  0.0000 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  5.56it/s]\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train | Loss:  8.2160 | Accuracy:  0.0000| Domain 1 Accuracy:  0.0000 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Val   | Loss:  8.2447 | Accuracy:  0.0200 | \n",
      "                            Domain 1 Accuracy:  0.0200 | Domain 2 Accuracy:     nan| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [00:02<00:00,  3.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m BERT(BERT_config) \u001b[38;5;66;03m# initialise the model\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# train the model (all cells except this one will print training log and evaluation at each batch)\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m pretrain_best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_pretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_dom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreval_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreval_dom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreval_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/1. University/2. Masters/7. Statistical Machine Learning/Assignments/A1/COMP90051-A1__Groupwork__Py/PrivatePackages/pytorch/model/model_class/__template__.py:290\u001b[0m, in \u001b[0;36mClassificationModel.fit_pretrain\u001b[0;34m(self, total_train_X, total_train_y, total_train_domain, total_train_mask, total_val_X, total_val_y, total_val_domain, total_val_mask)\u001b[0m\n\u001b[1;32m    287\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_criterion(pred, true)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mgrad_clip: \u001b[38;5;66;03m# gradient clip\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/COMP90051/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 256\n",
    "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
    "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
    "LOW_FREQ_TOKEN = False\n",
    "CLS = True\n",
    "PAD_FRONT = False\n",
    "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
    "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
    "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY) \n",
    "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
    "                                                              cropped_val_data, \\\n",
    "                                                                cropped_test_data, \\\n",
    "                                                                    cropped_future_data, \\\n",
    "                                                                        MAX_SENTENCE_LENGTH, \\\n",
    "                                                                            raw_token_pytorch_map, \\\n",
    "                                                                                CLS=CLS, \\\n",
    "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
    "                                                                                        pad_front=PAD_FRONT)\n",
    "pos_prior, neg_prior = get_distribution(train_y)\n",
    "print('class prior:', pos_prior, neg_prior)\n",
    "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
    "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
    "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
    "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
    "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
    "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
    "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
    "\n",
    "# pretrain_x = pretrain_x[:100]\n",
    "# pretrain_y = pretrain_y[:100]\n",
    "# pretrain_mask = pretrain_mask[:100]\n",
    "# pretrain_dom = pretrain_dom[:100]\n",
    "# preval_x = preval_x[:100]\n",
    "# preval_y = preval_y[:100]\n",
    "# preval_mask = preval_mask[:100]\n",
    "# preval_dom = preval_dom[:100]\n",
    "\n",
    "print('---')\n",
    "\n",
    "class BERT_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 256\n",
    "    d_ff = 512 # = 4* d_model\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    e_layers = 1\n",
    "    embedding_aggregation = 'cls' # TODO\n",
    "    n_mlp_layers = 1\n",
    "    res_learning = False\n",
    "    activation = nn.ReLU()\n",
    "    mask_flag = False # causal mask\n",
    "    train_embedding = True\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 8\n",
    "    epochs = 32\n",
    "    lr = 1e-5\n",
    "    patience = 3\n",
    "    loss = nn.BCELoss()\n",
    "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    validation_loss = nn.BCELoss()\n",
    "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    pretrain_loss = nn.CrossEntropyLoss()\n",
    "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
    "    regularisation_loss = None\n",
    "    scheduler = False\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = MAX_SENTENCE_LENGTH\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory + './'\n",
    "    saving_address = home_directory + f'./results/'\n",
    "    name = f'BERT_Classifier'\n",
    "\n",
    "model = BERT(BERT_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "pretrain_best_epoch = model.fit_pretrain(pretrain_x, pretrain_y, pretrain_dom, pretrain_mask, preval_x, preval_y, preval_dom, preval_mask)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval_pretrain(preval_x, preval_y, preval_dom, preval_mask, pretrain_best_epoch, evaluation_mode = True)\n",
    "\n",
    "# # train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "# best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
    "# print()\n",
    "\n",
    "# # as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "# model.load()\n",
    "# model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
    "# model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 1.203972804325936\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the predicted values (model outputs) and target values (ground truth labels)\n",
    "predicted_logits = torch.tensor([[np.log(0.1), np.log(0.9)], [np.log(0.1), np.log(0.9)]])  # Example predicted logits\n",
    "target_indices = torch.tensor([1, 0])  # Example target indices\n",
    "\n",
    "# Instantiate the CrossEntropyLoss function\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss_ce = criterion_ce(predicted_logits, target_indices)\n",
    "\n",
    "print(\"Cross Entropy Loss:\", loss_ce.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 2.2094946699280333\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the predicted values (model outputs) and target values (ground truth labels)\n",
    "predicted_logits = torch.tensor([[np.log(0.1/0.9), np.log(0.9/0.1)], [np.log(0.1/0.9), np.log(0.9/0.1)]])  # Example predicted logits\n",
    "target_indices = torch.tensor([1, 0])  # Example target indices\n",
    "\n",
    "# Instantiate the CrossEntropyLoss function\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss_ce = criterion_ce(predicted_logits, target_indices)\n",
    "\n",
    "print(\"Cross Entropy Loss:\", loss_ce.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 0.771100640296936\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the predicted values (model outputs) and target values (ground truth labels)\n",
    "predicted_logits = torch.tensor([[0.1, 0.9], [0.1, 0.9]])  # Example predicted logits (2 instances, 2 classes)\n",
    "target_indices = torch.tensor([1, 0])  # Example target indices\n",
    "\n",
    "# Instantiate the CrossEntropyLoss function\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss_ce = criterion_ce(predicted_logits, target_indices)\n",
    "\n",
    "print(\"Cross Entropy Loss:\", loss_ce.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the predicted values (model outputs) and target values (ground truth labels)\n",
    "predicted_logits = torch.tensor([[np.log(0.1), np.log(0.9)], [np.log(0.1), np.log(0.9)]])  # Example predicted logits\n",
    "target_indices = torch.tensor([1, 0])  # Example target indices\n",
    "\n",
    "# Instantiate the CrossEntropyLoss function\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss_ce = criterion_ce(predicted_logits, target_indices)\n",
    "\n",
    "print(\"Cross Entropy Loss:\", loss_ce.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross Entropy Loss: 1.2039728164672852\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the predicted values (model outputs) and target values (ground truth labels)\n",
    "predicted = torch.tensor([[0.1, 0.9], [0.1, 0.9]])  # Example predicted values after softmax\n",
    "target = torch.tensor([[0, 1], [1, 0]])            # Example target values\n",
    "\n",
    "# Instantiate the BCELoss function\n",
    "criterion_bce = nn.BCELoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss_bce = criterion_bce(predicted, target.float())  # Ensure target is of float type\n",
    "\n",
    "print(\"Binary Cross Entropy Loss:\", loss_bce.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross Entropy Loss: 1.2039726972579956\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the predicted values (model outputs) and target values (ground truth labels)\n",
    "predicted = torch.tensor([0.9, 0.9])  # Example predicted values after softmax\n",
    "target = torch.tensor([1, 0])            # Example target values\n",
    "\n",
    "# Instantiate the BCELoss function\n",
    "criterion_bce = nn.BCELoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss_bce = criterion_bce(predicted, target.float())  # Ensure target is of float type\n",
    "\n",
    "print(\"Binary Cross Entropy Loss:\", loss_bce.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM_DANN\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 512\n",
    "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
    "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
    "LOW_FREQ_TOKEN = False\n",
    "CLS = True\n",
    "PAD_FRONT = False\n",
    "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
    "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
    "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY) \n",
    "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
    "                                                              cropped_val_data, \\\n",
    "                                                                cropped_test_data, \\\n",
    "                                                                    cropped_future_data, \\\n",
    "                                                                        MAX_SENTENCE_LENGTH, \\\n",
    "                                                                            raw_token_pytorch_map, \\\n",
    "                                                                                CLS=CLS, \\\n",
    "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
    "                                                                                        pad_front=PAD_FRONT)\n",
    "pos_prior, neg_prior = get_distribution(train_y)\n",
    "print('class prior:', pos_prior, neg_prior)\n",
    "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
    "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
    "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
    "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
    "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
    "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
    "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
    "\n",
    "print('---')\n",
    "\n",
    "class LSTM_DANN_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 512\n",
    "    n_recurrent_layers = 2\n",
    "    bidirectional = False\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    n_mlp_clf_layers = 1\n",
    "    n_mlp_dom_layers = 1\n",
    "    flatten = False\n",
    "    activation = nn.ReLU()\n",
    "    res_learning = True\n",
    "    mask_flag = False # TODO\n",
    "    train_embedding = True\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 128\n",
    "    epochs = 32\n",
    "    lr = 1e-5\n",
    "    patience = 3\n",
    "    loss = nn.BCELoss()\n",
    "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    validation_loss = nn.BCELoss()\n",
    "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    domain_loss = nn.BCELoss()\n",
    "    pretrain_loss = nn.CrossEntropyLoss()\n",
    "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
    "    alpha = 0.1\n",
    "    regularisation_loss = None\n",
    "    scheduler = True\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = MAX_SENTENCE_LENGTH\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory + './'\n",
    "    saving_address = home_directory +  f'./results/'\n",
    "    name = f'LSTM_DANN'\n",
    "\n",
    "model = LSTM_DANN(LSTM_DANN_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
    "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 512\n",
    "MIN_FREQUENCY = 0 # because 40 is statistical sample requirement\n",
    "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
    "LOW_FREQ_TOKEN = False\n",
    "CLS = True\n",
    "PAD_FRONT = False\n",
    "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
    "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
    "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
    "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
    "                                                              cropped_val_data, \\\n",
    "                                                                cropped_test_data, \\\n",
    "                                                                    cropped_future_data, \\\n",
    "                                                                        MAX_SENTENCE_LENGTH, \\\n",
    "                                                                            raw_token_pytorch_map, \\\n",
    "                                                                                CLS=CLS, \\\n",
    "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
    "                                                                                        pad_front=PAD_FRONT)\n",
    "pos_prior, neg_prior = get_distribution(train_y)\n",
    "print('class prior:', pos_prior, neg_prior)\n",
    "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
    "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
    "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
    "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
    "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
    "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
    "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
    "\n",
    "print('---')\n",
    "\n",
    "class BERT_DANN_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 256\n",
    "    d_ff = 512 # = 4* d_model\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    e_layers = 3\n",
    "    embedding_aggregation = 'cls' # TODO\n",
    "    n_mlp_clf_layers = 1\n",
    "    n_mlp_dom_layers = 1\n",
    "    res_learning = False\n",
    "    activation = nn.ReLU()\n",
    "    mask_flag = False # causal mask\n",
    "    train_embedding = True\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 8\n",
    "    epochs = 32\n",
    "    lr = 1e-5\n",
    "    patience = 3\n",
    "    loss = nn.BCELoss()\n",
    "    # loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    validation_loss = nn.BCELoss()\n",
    "    # validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    domain_loss = nn.BCELoss()\n",
    "    pretrain_loss = nn.CrossEntropyLoss()\n",
    "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
    "    alpha = 0.1\n",
    "    regularisation_loss = None\n",
    "    scheduler = False\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = MAX_SENTENCE_LENGTH\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory + './'\n",
    "    saving_address = home_directory + f'./results/'\n",
    "    name = f'BERT_DANN'\n",
    "\n",
    "model = BERT_DANN(BERT_DANN_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
    "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DBCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT DBCE\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 256\n",
    "MIN_FREQUENCY = 40 # because 40 is statistical sample requirement\n",
    "MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE = False\n",
    "LOW_FREQ_TOKEN = False\n",
    "CLS = True\n",
    "PAD_FRONT = False\n",
    "W2V_CONTEXT_WINDOW = 5 # 2 to left, 2 to right\n",
    "cropped_train_data = crop_sentence_length(train_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = MAKE_CROPPED_REMAINS_INTO_NEW_INSTANCE)\n",
    "cropped_val_data = crop_sentence_length(val_data, max_sentence_length =  MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_test_data = crop_sentence_length(test_data, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "cropped_future_data = crop_sentence_length(data_test, max_sentence_length = MAX_SENTENCE_LENGTH, make_cropped_remains_into_new_instance = False)\n",
    "raw_token_pytorch_map = get_raw_token_pytorch_map(data = cropped_train_data, min_frequency = MIN_FREQUENCY)\n",
    "train_x, train_y, val_x, val_y, test_x, test_y, train_dom, val_dom, test_dom, future_x = Data_Factory(cropped_train_data, \\\n",
    "                                                              cropped_val_data, \\\n",
    "                                                                cropped_test_data, \\\n",
    "                                                                    cropped_future_data, \\\n",
    "                                                                        MAX_SENTENCE_LENGTH, \\\n",
    "                                                                            raw_token_pytorch_map, \\\n",
    "                                                                                CLS=CLS, \\\n",
    "                                                                                    low_freq_special_token=LOW_FREQ_TOKEN, \\\n",
    "                                                                                        pad_front=PAD_FRONT)\n",
    "pos_prior, neg_prior = get_distribution(train_y)\n",
    "print('class prior:', pos_prior, neg_prior)\n",
    "pos_dom_prior, neg_dom_prior = get_distribution(train_dom)\n",
    "print('domain prior:', pos_dom_prior, neg_dom_prior)\n",
    "dom1_pos_prior, dom1_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label1])\n",
    "print('dom1 class prior:', dom1_pos_prior, dom1_neg_prior)\n",
    "dom2_pos_prior, dom2_neg_prior = get_distribution([[0, 1] if label == 1 else [1, 0] for label in label2])\n",
    "print('dom2 class prior:', dom2_pos_prior, dom2_neg_prior)\n",
    "pretrain_x, pretrain_y, pretrain_mask, pretrain_dom, preval_x, preval_y, preval_mask, preval_dom = BERT_pretrain_DataFactory(train_data, val_data, SEED, raw_token_pytorch_map, MAX_SENTENCE_LENGTH)\n",
    "\n",
    "print('---')\n",
    "\n",
    "class BERT_DBCE_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 64\n",
    "    d_ff = 64 # = 4* d_model\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    e_layers = 1\n",
    "    embedding_aggregation = 'cls' # TODO\n",
    "    n_mlp_layers = 1\n",
    "    res_learning = False\n",
    "    activation = nn.ReLU()\n",
    "    mask_flag = False # causal mask\n",
    "    train_embedding = True\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    batch_size = 8\n",
    "    epochs = 32\n",
    "    lr = 1e-5\n",
    "    patience = 10\n",
    "    # loss = nn.BCELoss()\n",
    "    loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    # validation_loss = nn.BCELoss()\n",
    "    validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    # domain_1_loss = nn.BCELoss()\n",
    "    domain_1_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    # domain_1_validation_loss = nn.BCELoss()\n",
    "    domain_1_validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    # domain_2_loss = nn.BCELoss()\n",
    "    domain_2_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    # domain_2_validation_loss = nn.BCELoss()\n",
    "    domain_2_validation_loss = nn.BCELoss(weight=torch.FloatTensor([pos_prior, neg_prior]))\n",
    "    domain_prior = [pos_dom_prior, neg_dom_prior]\n",
    "    pretrain_loss = nn.CrossEntropyLoss()\n",
    "    pretrain_validation_loss = nn.CrossEntropyLoss()\n",
    "    regularisation_loss = None\n",
    "    scheduler = False\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    d_output = 2\n",
    "    seq_len = MAX_SENTENCE_LENGTH\n",
    "    n_unique_tokens = len(raw_token_pytorch_map)\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory + './'\n",
    "    saving_address = home_directory + f'./results/'\n",
    "    name = f'BERT_Classifier'\n",
    "\n",
    "model = BERT_DBCE(BERT_DBCE_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model.fit(train_x, train_y, train_dom, val_x, val_y, val_dom)\n",
    "print()\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model.load()\n",
    "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
    "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluation and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load()\n",
    "model.eval(val_x, val_y, val_dom, best_epoch, evaluation_mode = True)\n",
    "model.eval(test_x, test_y, test_dom, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = '2lstm_unidir_512d_8_512t40_wbce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_pred_y = model.predict(future_x)\n",
    "\n",
    "future_pred_y = [1 if x[1] > x[0] else 0 for x in future_pred_y]\n",
    "\n",
    "predictions = pd.DataFrame({'id': range(len(future_pred_y)), 'class': future_pred_y})\n",
    "predictions.to_csv(home_directory + f'predictions/{EXPERIMENT_NAME}_classification.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP90051",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
