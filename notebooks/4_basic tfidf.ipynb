{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from collections import defaultdict as dd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "\n",
    "import pickle\n",
    "import scipy\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "with open('../data/raw/domain1_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data1.append(json.loads(line))\n",
    "\n",
    "data2 = []\n",
    "with open('../data/raw/domain2_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data2.append(json.loads(line))\n",
    "\n",
    "data_test = []\n",
    "with open('../data/raw/test_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data_test.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'global_with_domain'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Val Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create domain labels for data\n",
    "for i in range(len(data1)):\n",
    "    data1[i]['domain'] = 1\n",
    "for i in range(len(data2)):\n",
    "    data2[i]['domain'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val Test Split\n",
    "\n",
    "# get labels for stratification\n",
    "label1 = [instance['label'] for instance in data1]\n",
    "label2 = [instance['label'] for instance in data2]\n",
    "\n",
    "train_ix_1, val_test_ix_1 = train_test_split(range(len(data1)), test_size=0.3, random_state=SEED, stratify = label1)\n",
    "train_ix_2, val_test_ix_2 = train_test_split(range(len(data2)), test_size=0.3, random_state=SEED, stratify = label2)\n",
    "val_ix_1, test_ix_1 = train_test_split(val_test_ix_1, test_size=0.5, random_state=SEED, stratify = [data1[i]['label'] for i in val_test_ix_1])\n",
    "val_ix_2, test_ix_2 = train_test_split(val_test_ix_2, test_size=0.5, random_state=SEED, stratify = [data2[i]['label'] for i in val_test_ix_2])\n",
    "\n",
    "# split data according to the index from train_test_split\n",
    "train_data_1 = [data1[i] for i in train_ix_1]\n",
    "val_data_1 = [data1[i] for i in val_ix_1]\n",
    "test_data_1 = [data1[i] for i in test_ix_1]\n",
    "train_data_2 = [data2[i] for i in train_ix_2]\n",
    "val_data_2 = [data2[i] for i in val_ix_2]\n",
    "test_data_2 = [data2[i] for i in test_ix_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # upsample the minority class (label 1) for domain 2 train data\n",
    "# train_data2_label0 = [d for d in train_data_2 if d['label'] == 0]\n",
    "# train_data2_label1 = [d for d in train_data_2 if d['label'] == 1]\n",
    "# train_data2_label1_upsampled = resample(train_data2_label1, replace=True, n_samples=len(train_data2_label0), random_state=SEED)\n",
    "\n",
    "# # Merge and shuffle the data back together\n",
    "# train_data2 = train_data2_label0 + train_data2_label1_upsampled\n",
    "# train_data2 = shuffle(train_data2, random_state=SEED)\n",
    "\n",
    "# len(train_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data from two domains together\n",
    "train_data = train_data_1 + train_data_2\n",
    "val_data = val_data_1 + val_data_2\n",
    "test_data = test_data_1 + test_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text, label, domain and id for each split\n",
    "train_text = [' '.join([str(encode) for encode in instance['text']]) for instance in train_data]\n",
    "val_text = [' '.join([str(encode) for encode in instance['text']]) for instance in val_data]\n",
    "test_text = [' '.join([str(encode) for encode in instance['text']]) for instance in test_data]\n",
    "future_text = [' '.join([str(encode) for encode in instance['text']]) for instance in data_test]\n",
    "\n",
    "train_label = [instance['label'] for instance in train_data]\n",
    "val_label = [instance['label'] for instance in val_data]\n",
    "test_label = [instance['label'] for instance in test_data]\n",
    "\n",
    "train_domain = [instance['domain'] for instance in train_data]\n",
    "val_domain = [instance['domain'] for instance in val_data]\n",
    "test_domain = [instance['domain'] for instance in test_data]\n",
    "\n",
    "train_id = list(range(len(train_data)))\n",
    "val_id = list(range(len(val_data)))\n",
    "test_id = list(range(len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "TFIDF = TfidfVectorizer(max_features=83581) # from EDA\n",
    "\n",
    "TFIDF.fit(train_text)\n",
    "train_tfidf = TFIDF.transform(train_text)\n",
    "val_tfidf = TFIDF.transform(val_text)\n",
    "test_tfidf = TFIDF.transform(test_text)\n",
    "future_tfidf = TFIDF.transform(future_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12600x68213 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1590593 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12600x1 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12600 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix(train_domain).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = hstack([train_tfidf, csr_matrix(train_domain).transpose()])\n",
    "val_tfidf = hstack([val_tfidf, csr_matrix(val_domain).transpose()])\n",
    "test_tfidf = hstack([test_tfidf, csr_matrix(test_domain).transpose()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "# BoW = CountVectorizer(max_features=83581) # from EDA\n",
    "\n",
    "# BoW.fit(train_text)\n",
    "# train_bow = BoW.transform(train_text)\n",
    "# val_bow = BoW.transform(val_text)\n",
    "# test_bow = BoW.transform(test_text)\n",
    "# future_bow = BoW.transform(future_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "save_npz(f\"../data/curated/baseline/train_tfidf_{EXPERIMENT}_x.npz\", train_tfidf)\n",
    "save_npz(f\"../data/curated/baseline/val_tfidf_{EXPERIMENT}_x.npz\", val_tfidf)\n",
    "save_npz(f\"../data/curated/baseline/test_tfidf_{EXPERIMENT}_x.npz\", test_tfidf)\n",
    "save_npz(f'../data/curated/baseline/future_tfidf_{EXPERIMENT}_x.npz', future_tfidf)\n",
    "\n",
    "# save_npz(\"../data/curated/baseline/train_bow_oversample_x.npz\", train_bow)\n",
    "# save_npz(\"../data/curated/baseline/val_bow_x.npz\", val_bow)\n",
    "# save_npz(\"../data/curated/baseline/test_bow_x.npz\", test_bow)\n",
    "# save_npz('../data/curated/baseline/future_bow_x.npz', future_bow)\n",
    "\n",
    "with open(f\"../data/curated/baseline/train_{EXPERIMENT}_y.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_label, f)\n",
    "with open(f\"../data/curated/baseline/val_{EXPERIMENT}_y.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_label, f)\n",
    "with open(f\"../data/curated/baseline/test_{EXPERIMENT}_y.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_label, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, gap:int=0):\n",
    "\n",
    "    \"\"\"\n",
    "        Gets sequential feature importance of model and returns it in a list format\n",
    "\n",
    "        Input:\n",
    "            - model: model which has been fitted - must have feature_importances_ and feature_names_in_ attributes\n",
    "            - gap: int - how many features to include in each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    ordered_feature_importance = {}\n",
    "\n",
    "    feature_importance = list(model.feature_importances_)\n",
    "    feature_importance_list = [(i, feature_importance[i]) for i in range(len(feature_importance))]\n",
    "    feature_importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    feature_importance_list = [x for x in feature_importance_list if x[1] > 0]\n",
    "\n",
    "    curr = []\n",
    "    score = 0\n",
    "    for i in tqdm(range(len(feature_importance_list))):\n",
    "\n",
    "        curr.append(feature_importance_list[i][0])\n",
    "        score += feature_importance_list[i][1]\n",
    "\n",
    "\n",
    "        if (i+1) % gap == 0:\n",
    "            ordered_feature_importance[tuple(curr)] = score\n",
    "    \n",
    "    if (i+1) % gap != 0: # account for last combo (if it doesn't fit into the gap)\n",
    "        ordered_feature_importance[tuple(curr)] = score\n",
    "\n",
    "    return ordered_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 955/955 [00:00<00:00, 321861.01it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf_xgb = XGBClassifier(max_depth = 12, \n",
    "                            random_state = SEED, \n",
    "                            n_estimators = 100,\n",
    "                            colsample_bytree = 0.75,\n",
    "                            subsample = 0.75\n",
    "                            )\n",
    "\n",
    "tfidf_xgb.fit(train_tfidf, train_label)\n",
    "\n",
    "xgb_tfidf_feature_importance_ordering = get_feature_importance(tfidf_xgb, 10) \n",
    "\n",
    "# export\n",
    "with open(f'../models/xgb_tfidf_feature_importance_ordering_{EXPERIMENT}.pickle', 'wb') as f:\n",
    "    pickle.dump(xgb_tfidf_feature_importance_ordering, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_xgb = XGBClassifier(max_depth = 12, \n",
    "#                             random_state = SEED, \n",
    "#                             n_estimators = 100,\n",
    "#                             colsample_bytree = 0.75,\n",
    "#                             subsample = 0.75\n",
    "#                             )\n",
    "\n",
    "# bow_xgb.fit(train_bow, train_label)\n",
    "\n",
    "# xgb_bow_feature_importance_ordering = get_feature_importance(bow_xgb, 10) \n",
    "\n",
    "# # export\n",
    "# with open(f'../models/xgb_bow_feature_importance_ordering_{EXPERIMENT}.pickle', 'wb') as f:\n",
    "#     pickle.dump(xgb_bow_feature_importance_ordering, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Inference 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_importance_ordering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(train_tfidf[:, \u001b[38;5;28mlist\u001b[39m(\u001b[43mfeature_importance_ordering\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_importance_ordering' is not defined"
     ]
    }
   ],
   "source": [
    "len(train_tfidf[:, list(feature_importance_ordering.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features_ is 68213 and input n_features is 924",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[81], line 8\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m     feature_importance_ordering \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[1;32m      6\u001b[0m NUM_FEATURE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m92\u001b[39m\n",
      "\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(train_label, \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tfidf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_importance_ordering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mNUM_FEATURE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m), \n",
      "\u001b[1;32m      9\u001b[0m       f1_score(train_label, clf\u001b[38;5;241m.\u001b[39mpredict(train_tfidf[:, \u001b[38;5;28mlist\u001b[39m(feature_importance_ordering\u001b[38;5;241m.\u001b[39mkeys())[NUM_FEATURE]]), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m), \n",
      "\u001b[1;32m     10\u001b[0m     balanced_accuracy_score(train_label, clf\u001b[38;5;241m.\u001b[39mpredict(train_tfidf[:, \u001b[38;5;28mlist\u001b[39m(feature_importance_ordering\u001b[38;5;241m.\u001b[39mkeys())[NUM_FEATURE]])))\n",
      "\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(val_label, clf\u001b[38;5;241m.\u001b[39mpredict(val_tfidf[:, \u001b[38;5;28mlist\u001b[39m(feature_importance_ordering\u001b[38;5;241m.\u001b[39mkeys())[NUM_FEATURE]])), \n",
      "\u001b[1;32m     12\u001b[0m       f1_score(val_label, clf\u001b[38;5;241m.\u001b[39mpredict(val_tfidf[:, \u001b[38;5;28mlist\u001b[39m(feature_importance_ordering\u001b[38;5;241m.\u001b[39mkeys())[NUM_FEATURE]]), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m), \n",
      "\u001b[1;32m     13\u001b[0m     balanced_accuracy_score(val_label, clf\u001b[38;5;241m.\u001b[39mpredict(val_tfidf[:, \u001b[38;5;28mlist\u001b[39m(feature_importance_ordering\u001b[38;5;241m.\u001b[39mkeys())[NUM_FEATURE]])))\n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(test_label, clf\u001b[38;5;241m.\u001b[39mpredict(test_tfidf[:, \u001b[38;5;28mlist\u001b[39m(feature_importance_ordering\u001b[38;5;241m.\u001b[39mkeys())[NUM_FEATURE]])), \n",
      "\u001b[1;32m     15\u001b[0m       f1_score(test_label, clf\u001b[38;5;241m.\u001b[39mpredict(test_tfidf[:, \u001b[38;5;28mlist\u001b[39m(feature_importance_ordering\u001b[38;5;241m.\u001b[39mkeys())[NUM_FEATURE]]), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m),\n",
      "\u001b[1;32m     16\u001b[0m     balanced_accuracy_score(test_label, clf\u001b[38;5;241m.\u001b[39mpredict(test_tfidf[:, \u001b[38;5;28mlist\u001b[39m(feature_importance_ordering\u001b[38;5;241m.\u001b[39mkeys())[NUM_FEATURE]])))\n",
      "\n",
      "File \u001b[0;32m/Library/anaconda3/envs/ADS/lib/python3.11/site-packages/lightgbm/sklearn.py:1178\u001b[0m, in \u001b[0;36mLGBMClassifier.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n",
      "\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[1;32m   1168\u001b[0m     X: _LGBM_ScikitMatrixLike,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1175\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n",
      "\u001b[1;32m   1176\u001b[0m ):\n",
      "\u001b[1;32m   1177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1186\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objective) \u001b[38;5;129;01mor\u001b[39;00m raw_score \u001b[38;5;129;01mor\u001b[39;00m pred_leaf \u001b[38;5;129;01mor\u001b[39;00m pred_contrib:\n",
      "\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\n",
      "File \u001b[0;32m/Library/anaconda3/envs/ADS/lib/python3.11/site-packages/lightgbm/sklearn.py:1208\u001b[0m, in \u001b[0;36mLGBMClassifier.predict_proba\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\n",
      "\u001b[1;32m   1197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[1;32m   1198\u001b[0m     X: _LGBM_ScikitMatrixLike,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1205\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n",
      "\u001b[1;32m   1206\u001b[0m ):\n",
      "\u001b[1;32m   1207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objective) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (raw_score \u001b[38;5;129;01mor\u001b[39;00m pred_leaf \u001b[38;5;129;01mor\u001b[39;00m pred_contrib):\n",
      "\u001b[1;32m   1219\u001b[0m         _log_warning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot compute class probabilities or labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1220\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to the usage of customized objective function.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1221\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning raw scores instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/Library/anaconda3/envs/ADS/lib/python3.11/site-packages/lightgbm/sklearn.py:894\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    892\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m!=\u001b[39m n_features:\n",
      "\u001b[0;32m--> 894\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features of the model must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    895\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch the input. Model n_features_ is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    896\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput n_features is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m    897\u001b[0m \u001b[38;5;66;03m# retrive original params that possibly can be used in both training and prediction\u001b[39;00m\n",
      "\u001b[1;32m    898\u001b[0m \u001b[38;5;66;03m# and then overwrite them (considering aliases) with params that were passed directly in prediction\u001b[39;00m\n",
      "\u001b[1;32m    899\u001b[0m predict_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features_ is 68213 and input n_features is 924"
     ]
    }
   ],
   "source": [
    "with open(f'../models/tmp_models/jiaochengb-balaccu_lgbc_xgb_tfidf_{EXPERIMENT}_Baseline.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "with open(f'../models/xgb_tfidf_feature_importance_ordering_{EXPERIMENT}.pickle', 'rb') as f:\n",
    "    feature_importance_ordering = pickle.load(f)\n",
    "\n",
    "NUM_FEATURE = 92\n",
    "\n",
    "print(accuracy_score(train_label, clf.predict(train_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]])), \n",
    "      f1_score(train_label, clf.predict(train_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]]), average='binary'), \n",
    "    balanced_accuracy_score(train_label, clf.predict(train_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]])))\n",
    "print(accuracy_score(val_label, clf.predict(val_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]])), \n",
    "      f1_score(val_label, clf.predict(val_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]]), average='binary'), \n",
    "    balanced_accuracy_score(val_label, clf.predict(val_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]])))\n",
    "print(accuracy_score(test_label, clf.predict(test_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]])), \n",
    "      f1_score(test_label, clf.predict(test_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]]), average='binary'),\n",
    "    balanced_accuracy_score(test_label, clf.predict(test_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]])))\n",
    "\n",
    "future_predict = clf.predict(future_tfidf[:, list(feature_importance_ordering.keys())[NUM_FEATURE]])\n",
    "predictions = pd.DataFrame({'id': range(len(future_predict)), 'class': future_predict})\n",
    "predictions.to_csv(f'../predictions/jiaochengb_lgbc_xgb_tfidf_{EXPERIMENT}_Baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "1.0 1.0 1.0\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.8681481481481481 0.6641509433962265 0.7676190476190476\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.8688888888888889 0.6685393258426966 0.7710714285714286\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "with open(f'../models/tmp_models/jiaochengb-balaccu_lgbc_xgb_tfidf_{EXPERIMENT}_Baseline.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "with open(f'../models/xgb_tfidf_feature_importance_ordering_{EXPERIMENT}.pickle', 'rb') as f:\n",
    "    feature_importance_ordering = pickle.load(f)\n",
    "\n",
    "NUM_FEATURE = 92\n",
    "\n",
    "print(accuracy_score(train_label, clf.predict(train_tfidf[:])), \n",
    "      f1_score(train_label, clf.predict(train_tfidf[:]), average='binary'), \n",
    "    balanced_accuracy_score(train_label, clf.predict(train_tfidf[:])))\n",
    "print(accuracy_score(val_label, clf.predict(val_tfidf[:])), \n",
    "      f1_score(val_label, clf.predict(val_tfidf[:]), average='binary'), \n",
    "    balanced_accuracy_score(val_label, clf.predict(val_tfidf[:])))\n",
    "print(accuracy_score(test_label, clf.predict(test_tfidf[:])), \n",
    "      f1_score(test_label, clf.predict(test_tfidf[:]), average='binary'),\n",
    "    balanced_accuracy_score(test_label, clf.predict(test_tfidf[:])))\n",
    "\n",
    "future_predict = clf.predict(future_tfidf[:])\n",
    "predictions = pd.DataFrame({'id': range(len(future_predict)), 'class': future_predict})\n",
    "predictions.to_csv(f'../predictions/jiaochengb_lgbc_xgb_tfidf_{EXPERIMENT}_Baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "0.7706666666666666 0.6413526570048309 0.7060096618357488 0.7676190476190476\n",
      "0.7706666666666666 0.6322705314009662 0.7014685990338164 0.7710714285714286\n",
      "[LightGBM] [Warning] Unknown parameter: warnings\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "with open(f'../models/tmp_models/jiaochengb-balaccu_lgbc_xgb_tfidf_{EXPERIMENT}_Baseline.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "with open(f'../models/xgb_tfidf_feature_importance_ordering_{EXPERIMENT}.pickle', 'rb') as f:\n",
    "    feature_importance_ordering = pickle.load(f)\n",
    "\n",
    "val_pred = clf.predict(val_tfidf[:])\n",
    "test_pred = clf.predict(test_tfidf[:])\n",
    "\n",
    "val_dom1_pred = [val_pred[i] for i in range(len(val_pred)) if val_domain[i] == 1]\n",
    "val_dom2_pred = [val_pred[i] for i in range(len(val_pred)) if val_domain[i] == 2]\n",
    "val_dom1_true = [val_label[i] for i in range(len(val_label)) if val_domain[i] == 1]\n",
    "val_dom2_true = [val_label[i] for i in range(len(val_label)) if val_domain[i] == 2]\n",
    "\n",
    "test_dom1_pred = [test_pred[i] for i in range(len(test_pred)) if test_domain[i] == 1]\n",
    "test_dom2_pred = [test_pred[i] for i in range(len(test_pred)) if test_domain[i] == 2]\n",
    "test_dom1_true = [test_label[i] for i in range(len(test_label)) if test_domain[i] == 1]\n",
    "test_dom2_true = [test_label[i] for i in range(len(test_label)) if test_domain[i] == 2]\n",
    "\n",
    "\n",
    "print(balanced_accuracy_score(val_dom1_true, val_dom1_pred), balanced_accuracy_score(val_dom2_true, val_dom2_pred), \\\n",
    "      (balanced_accuracy_score(val_dom1_true, val_dom1_pred) + balanced_accuracy_score(val_dom2_true, val_dom2_pred)) / 2, balanced_accuracy_score(val_label, val_pred))\n",
    "print(balanced_accuracy_score(test_dom1_true, test_dom1_pred), balanced_accuracy_score(test_dom2_true, test_dom2_pred), \\\n",
    "        (balanced_accuracy_score(test_dom1_true, test_dom1_pred) + balanced_accuracy_score(test_dom2_true, test_dom2_pred)) / 2, balanced_accuracy_score(test_label, test_pred))\n",
    "\n",
    "future_predict = clf.predict(future_tfidf[:])\n",
    "predictions = pd.DataFrame({'id': range(len(future_predict)), 'class': future_predict})\n",
    "predictions.to_csv('../predictions/jiaochengb-balaccu_lgbc_xgb_tfidf_Baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864444"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"../models/tuning_results/jiaochengb-balaccu_lgbc_xgb_tfidf_{EXPERIMENT}_Baseline.csv\")\n",
    "df['Val accu'].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
