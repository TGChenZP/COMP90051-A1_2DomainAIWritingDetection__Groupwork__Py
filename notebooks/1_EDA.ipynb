{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "with open('../data/raw/comp90051-2024s1-project-1/domain1_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data1.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = []\n",
    "with open('../data/raw/comp90051-2024s1-project-1/domain2_train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data2.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data: 5000 13000\n"
     ]
    }
   ],
   "source": [
    "print('length of data:', len(data1), len(data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique symbols: 83581\n",
      "number of unique symbols in d1: 83581\n",
      "number of unique symbols in d2: 71481\n"
     ]
    }
   ],
   "source": [
    "# number of unique words, and their min max value\n",
    "all_symbols = set()\n",
    "d1_symbols = set()\n",
    "d2_symbols = set()\n",
    "for i in range(len(data1)):\n",
    "    all_symbols.update(set(data1[i]['text']))\n",
    "    d1_symbols.update(set(data1[i]['text']))\n",
    "for j in range(len(data2)):\n",
    "    all_symbols.update(set(data2[j]['text']))\n",
    "    d2_symbols.update(set(data2[j]['text']))\n",
    "\n",
    "print('number of unique symbols:', len(all_symbols))\n",
    "all_symbols_list = list(all_symbols)\n",
    "all_symbols_list.sort()\n",
    "\n",
    "print('number of unique symbols in d1:', len(all_symbols))\n",
    "d1_symbols_list = list(d1_symbols)\n",
    "d1_symbols_list.sort()\n",
    "\n",
    "print('number of unique symbols in d2:', len(d2_symbols))\n",
    "d2_symbols_list = list(d2_symbols)\n",
    "d2_symbols_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens in all sentences: 4341949\n",
      "average length of sentence: 241.2193888888889\n",
      "max length of sentence: 1982\n",
      "min length of sentence: 6\n",
      "---\n",
      "total tokens in domain1: 908261\n",
      "average length of sentence for domain 1: 181.6522\n",
      "max length of sentence for domain 1: 1982\n",
      "min length of sentence for domain 1: 6\n",
      "---\n",
      "total tokens in domain2: 3433688\n",
      "average length of sentence for domain 2: 264.12984615384613\n",
      "max length of sentence for domain 2: 1100\n",
      "min length of sentence for domain 2: 15\n"
     ]
    }
   ],
   "source": [
    "# total length of tokens\n",
    "total_length = 0\n",
    "total_length_1 = 0\n",
    "total_length_2 = 0\n",
    "\n",
    "max_length = 0\n",
    "max_length_1 = 0\n",
    "max_length_2 = 0\n",
    "min_length = 1000000000\n",
    "min_length_1 = 1000000000\n",
    "min_length_2 = 1000000000\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    total_length += len(data1[i]['text'])\n",
    "    total_length_1 += len(data1[i]['text'])\n",
    "    if len(data1[i]['text']) > max_length_1:\n",
    "        max_length_1 = len(data1[i]['text'])\n",
    "    if len(data1[i]['text']) < min_length_1:\n",
    "        min_length_1 = len(data1[i]['text'])\n",
    "for j in range(len(data2)):\n",
    "    total_length += len(data2[j]['text'])\n",
    "    total_length_2 += len(data2[j]['text'])\n",
    "    if len(data2[j]['text']) > max_length_2:\n",
    "        max_length_2 = len(data2[j]['text'])\n",
    "    if len(data2[j]['text']) < min_length_2:\n",
    "        min_length_2 = len(data2[j]['text'])\n",
    "print('total tokens in all sentences:', total_length)\n",
    "print('average length of sentence:', total_length / (len(data1) + len(data2)))\n",
    "print('max length of sentence:', max(max_length_1, max_length_2))\n",
    "print('min length of sentence:', min(min_length_1, min_length_2))\n",
    "print('---')\n",
    "\n",
    "# number of sentences\n",
    "print('total tokens in domain1:', total_length_1)\n",
    "print('average length of sentence for domain 1:', total_length_1 / len(data1))\n",
    "print('max length of sentence for domain 1:', max_length_1)\n",
    "print('min length of sentence for domain 1:', min_length_1)\n",
    "print('---')\n",
    "print('total tokens in domain2:', total_length_2)\n",
    "print('average length of sentence for domain 2:', total_length_2 / len(data2))\n",
    "print('max length of sentence for domain 2:', max_length_2)\n",
    "print('min length of sentence for domain 2:', min_length_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of words\n",
    "word_freq = dict()\n",
    "word_freq_1 = dict()\n",
    "word_freq_2 = dict()\n",
    "for i in range(len(data1)):\n",
    "    for word in data1[i]['text']:\n",
    "        if word not in word_freq_1:\n",
    "            word_freq_1[word] = 1\n",
    "        else:\n",
    "            word_freq_1[word] += 1\n",
    "\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] += 1\n",
    "\n",
    "for i in range(len(data2)):\n",
    "    for word in data2[i]['text']:\n",
    "        if word not in word_freq_2:\n",
    "            word_freq_2[word] = 1\n",
    "        else:\n",
    "            word_freq_2[word] += 1\n",
    "        \n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] += 1\n",
    "\n",
    "word_freq_1 = sorted(word_freq_1.items(), key=lambda x: x[1], reverse=True)\n",
    "word_freq_2 = sorted(word_freq_2.items(), key=lambda x: x[1], reverse=True)\n",
    "word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word frequency\n",
      "count    83581.000000\n",
      "mean        51.948996\n",
      "std       1416.562614\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          5.000000\n",
      "max     207665.000000\n",
      "       word frequency\n",
      "count    23938.000000\n",
      "mean        37.942226\n",
      "std        628.635212\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          6.000000\n",
      "max      59283.000000\n",
      "       word frequency\n",
      "count    71481.000000\n",
      "mean        48.036373\n",
      "std       1209.181410\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          4.000000\n",
      "max     160016.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.DataFrame({'word frequency': [x[1] for x in word_freq]}).describe())\n",
    "print(pd.DataFrame({'word frequency': [x[1] for x in word_freq_1]}).describe())\n",
    "print(pd.DataFrame({'word frequency': [x[1] for x in word_freq_2]}).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38338 12535 32058\n"
     ]
    }
   ],
   "source": [
    "print(len([x for x in word_freq if x[1] > 1]), len([x for x in word_freq_1 if x[1] > 1]), len([x for x in word_freq_2 if x[1] > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13677 4126 11404\n"
     ]
    }
   ],
   "source": [
    "print(len([x for x in word_freq if x[1] > 10]), len([x for x in word_freq_1 if x[1] > 10]), len([x for x in word_freq_2 if x[1] > 10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP90051",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
